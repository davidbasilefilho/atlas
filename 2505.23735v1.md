# arXiv:2505.23735v1 [cs.CL] 29 May 2025

## Atlas: Learning to Optimally Memorize the Context at Test Time

### Ali Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong,

### Meisam Razaviyayn, and Vahab Mirrokni

```
Abstract
Transformers have been established as the most popular backbones in sequence modeling, mainly due to their
effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity,
however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative
architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent
success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to
longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory
capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update,
i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size
memory. To enhance all these three aspects, we presentAtlas, a long-term memory module with high capacity that learns
to memorize thecontextby optimizing the memory based on the current and past tokens, overcoming the online nature of
long-term memory models. Building on this insight, we present a new family of Transformer-like architectures, called
DeepTransformers, that are strict generalizations of the original Transformer architecture. Our experimental results on
language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show thatAtlas
surpasses the performance of Transformers and recent linear recurrent models.Atlasfurther improves the long context
performance of Titans, achieving +80% accuracy in 10M context length of BABILong benchmark.
```
## 1 Introduction

```
The attention module (Bahdanau et al. 2014) is a critical building block in modern deep learning architectures (Achiam
et al. 2023; Behrouz, Zhong, et al. 2024; Kamath et al. 2025; Vaswani et al. 2017), excelling due to its scalability and
performance in in-context retrieval tasks. In principle, attention functions as an associative memory, computing direct
pairwise token dependencies to store key-value mappings and retrieve them via query-key similarities. Computing this
pairwise dependencies, however, while accurate, causes quadratic space and time complexity, limiting their applicability in
long context understanding, memorization, or modeling (Dalal et al. 2025; Li, Huang, et al. 2024; Liu, Lin, et al. 2024).
Recent research efforts aim to overcome the limitations of Transformersâ€”i.e., pure attention-based architecturesâ€”in
long-context modeling by designing more efficient yet effective recurrent neural networks (Behrouz, Zhong, et al. 2024;
Peng, Zhang, et al. 2025; Schlag et al. 2021). These modern recurrent architectures can be unified as associative memory
modules optimizing an internal objective termed â€™attentional biasâ€™ (Behrouz, Razaviyayn, et al. 2025). Unlike Transformersâ€™
growing KV cache, these models use fixed-size memory, necessitating improved memory management. Consequently,
thereâ€™s growing interest in enhancing RNN memory management through more effective: (i) Learning rules, from additive
learning (Katharopoulos et al. 2020) to DeltaNetâ€™s Delta rule (Schlag et al. 2021); (ii) Forget (Retention) Gates, from RetNetâ€™s
input-independent gating (Sun, Dong, et al. 2023) to adaptive gating in Titans (Behrouz, Zhong, et al. 2024) and RWKV-
7 (Peng, Zhang, et al. 2025); and (iii) Memory Architectures, from vector-valued memory (Peng, Alcaide, et al. 2023; Sun,
Dong, et al. 2023) to neural deep memory modules (Behrouz, Zhong, et al. 2024; Sun, Li, et al. 2024).
Despite the success of these improved models in a diverse set of downstream benchmarks, they often struggle with long
context understanding, in-context retrieval, and extrapolation to longer sequences (Arora, Eyuboglu, Zhang, et al. 2024;
âˆ—{alibehrouz, zemanli, pkacham, dengyuan, peilinz, razaviyayn, mirrokni}@google.com, and majiddl.2099@gmail.com
```

```
Table 1: A summary of the recent modern recurrent neural networks. We compare these architectures based on five
characteristics: (1) Dynamic decay; (2) Deep neural memory; (3) non-linear memory capacity; (4) Locally optimal: managing
memory by (approximating) the second-order information about tokens; (5) Flexible context: the ability to flexibly memorize
the context.ğœ™(Â·)andğœ™âˆ—(Â·)represent polynomial and infinite-dimensional feature mappings (see Equation 22).
```
Model Attentional Biasâ„“(Â·;Â·) Optimizer DynamicDecay MemoryDeep Non-linearCapacityâ€  OptimalLocally FlexibleContext Memory Write Operation

Attention Ãğ¿ğ‘¡= 1 ğ‘ğ‘¡âˆ¥Mkğ‘¡âˆ’vğ‘¡âˆ¥^22 NPâ€¡ âœ—âœ— âœ“ âœ“ âœ— Mğ‘¡=Mğ‘¡âˆ’ 1 âˆª{(kğ‘¡,vğ‘¡)}
SWA Ãğ¿ğ‘¡=ğ‘ğ‘ğ‘¡âˆ¥Mkğ‘¡âˆ’vğ‘¡âˆ¥^22 NP âœ—âœ— âœ“ âœ“ âœ“ Mğ‘¡=(Mğ‘¡âˆ’ 1 \{(kğ‘,vğ‘)})âˆª{(kğ‘¡,vğ‘¡)}
Linear Attention âŸ¨Mğ‘¡kğ‘¡,vğ‘¡âŸ© GD âœ—âœ—âœ—âœ—âœ— Mğ‘¡=Mğ‘¡âˆ’ 1 +vğ‘¡kâŠ¤ğ‘¡
RetNet âŸ¨Mğ‘¡kğ‘¡,vğ‘¡âŸ© GD âœ—âœ—âœ—âœ—âœ— Mğ‘¡=ğ›¼Mğ‘¡âˆ’ 1 +vğ‘¡kâŠ¤ğ‘¡
GLA âŸ¨Mğ‘¡kğ‘¡,vğ‘¡âŸ© GD âœ“ âœ—âœ—âœ—âœ— Mğ‘¡=Diag(ğ›¼ğ‘¡)Mğ‘¡âˆ’ 1 +vğ‘¡kâŠ¤ğ‘¡
PolySketchFor. âŸ¨Mğ‘¡kğ‘ğ‘¡,vğ‘¡âŸ© GD âœ—âœ— âœ“ âœ—âœ— Mğ‘¡=Mğ‘¡âˆ’ 1 +vğ‘¡(kâŠ¤ğ‘¡)ğ‘
TTT âˆ¥Mğ‘¡(kğ‘¡)âˆ’vğ‘¡âˆ¥^22 GD âœ— âœ“ âœ—âœ—âœ— Mğ‘¡=Mğ‘¡âˆ’ 1 âˆ’ğœ‚âˆ‡â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡)
DeltaNet âˆ¥Mğ‘¡kğ‘¡âˆ’vğ‘¡âˆ¥^22 GD âœ—âœ—âœ—âœ—âœ— Mğ‘¡=(Iâˆ’ğ›½ğ‘¡kğ‘¡kâŠ¤ğ‘¡)Mğ‘¡âˆ’ 1 +ğ›½ğ‘¡vğ‘¡kâŠ¤ğ‘¡
Longhorn âˆ¥Mğ‘¡kğ‘¡âˆ’vğ‘¡âˆ¥^22 Implicit GD âœ—âœ—âœ—âœ—âœ— Mğ‘¡=(Iâˆ’ğ›¿ğ‘¡kğ‘¡kâŠ¤)Mğ‘¡âˆ’ 1 +(ğ›¿ğ‘¡âŠ™vğ‘¡)kğ‘¡Â§
Gated DeltaNet âˆ¥Mğ‘¡kğ‘¡âˆ’vğ‘¡âˆ¥^22 GD âœ“ âœ—âœ—âœ—âœ— Mğ‘¡=ğ›¼ğ‘¡(Iâˆ’ğ›½ğ‘¡kğ‘¡kâŠ¤ğ‘¡)Mğ‘¡âˆ’ 1 +ğ›½ğ‘¡vğ‘¡kâŠ¤ğ‘¡
RWKV-7 âˆ¥Mğ‘¡kğ‘¡âˆ’vğ‘¡âˆ¥^22 GD âœ“ âœ—âœ—âœ—âœ— Mğ‘¡=(diag(ğ›¼ğ‘¡)âˆ’ğ›½ğ‘¡kğ‘¡kâŠ¤ğ‘¡)Mğ‘¡âˆ’ 1 +ğ›½ğ‘¡vğ‘¡kâŠ¤ğ‘¡

Titans âˆ¥Mğ‘¡(kğ‘¡)âˆ’vğ‘¡âˆ¥^22 GD w/ M.âˆ— âœ“ âœ“ âœ—âœ—âœ— MSğ‘¡ğ‘¡==ğœ‚ğ›¼ğ‘¡Sğ‘¡Mğ‘¡âˆ’ 1 ğ‘¡âˆ’âˆ’^1 ğœƒ+Sğ‘¡âˆ‡ğ‘¡â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡)

Titansâ€“ âˆ¥Mğ‘¡(kğ‘¡)âˆ’vğ‘¡âˆ¥^22 GD âœ“ âœ“ âœ—âœ—âœ— Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡)
Moneta âˆ¥Mğ‘¡(kğ‘¡)âˆ’vğ‘¡âˆ¥ğ‘ğ‘ GD âœ“ âœ“ âœ“ âœ—âœ— Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡â„“(Mğ‘–âˆ’ 1 ;kğ‘¡,vğ‘¡)
Memora âˆ¥Mğ‘¡(kğ‘¡)âˆ’vğ‘¡âˆ¥^22 GD âœ“ âœ“ âœ—âœ—âœ— Mğ‘¡=ğœ(ğ›¼ğ‘¡log(Mğ‘¡âˆ’ 1 )âˆ’ğœ‚ğ‘¡âˆ‡â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡))
Our Models
DLA âŸ¨Mğ‘¡(ğœ™(kğ‘¡)),vğ‘¡âŸ© GD âœ“ âœ“ âœ—âœ—âœ— Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡)
DeepTransformerâ‹‡ âŸ¨Mğ‘¡(ğœ™âˆ—(kğ‘¡)),vğ‘¡âŸ© GD âœ“ âœ“ âœ“ âœ—âœ— Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡)
SWDT Ãğ¿ğ‘–=ğ‘âŸ¨Mğ‘¡(ğœ™âˆ—(kğ‘–)),vğ‘–âŸ© GD âœ“ âœ“ âœ“ âœ— âœ“ Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡)
OmegaNet Ãğ¿ğ‘–=ğ‘ğ›¾ğ‘–âˆ¥Mğ‘¡(ğœ™(kğ‘–))âˆ’vğ‘–âˆ¥^22 GD âœ“ âœ“ âœ“ âœ— âœ“ Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡)
Dotâ‹‡ Ãğ¿ğ‘–=ğ‘ğ›¾ğ‘–âˆ¥Mğ‘¡(ğœ™âˆ—(kğ‘–))âˆ’vğ‘–âˆ¥^22 GD âœ“ âœ“ âœ“ âœ— âœ“ Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡)

Atlas Ãğ¿ğ‘–=ğ‘ğ›¾ğ‘–âˆ¥Mğ‘¡(ğœ™(kğ‘–))âˆ’vğ‘–âˆ¥^22 Muon âœ“ âœ“ âœ“ âœ“ âœ“ MSğ‘¡ğ‘¡==ğœƒğ›¼ğ‘¡Sğ‘¡Mğ‘¡âˆ’ 1 ğ‘¡âˆ’âˆ’âˆ‡^1 âˆ’â„“ğœ‚(Mğ‘¡NS-5ğ‘¡âˆ’ 1 ;k(Sğ‘¡,ğ‘¡v)ğ‘¡)

â€ The matrix-valued memory version is considered. â€¡NP: Nonparametric Â§ğ›¿ğ‘¡= 1 +ğ›½ğ›½ğ‘¡
ğ‘¡kâŠ¤ğ‘¡kğ‘¡.
âˆ—Gradient Descent with Momentum. â‹‡Without Normalization.

```
Behrouz, Zhong, et al. 2024; Wen et al. 2024; Yang, Kautz, et al. 2024). We observe these shortcomings arise from three
design aspects: (1) The online nature of their memory update, where memory is optimized based on the current token
while retaining past memory state, leading to memorization of individual tokens without considering broader context;
(2) The limited capacity of memory, where architecture and key-value feature mappings restrict the number of perfectly
mappable key-value pairs; and (3) The expressiveness of memory management (i.e., the internal objectiveâ€™s optimizer), as
most recent models use gradient descent that relies on the first-order information about the dynamics of tokens, causing
the memory to converge to spurious local minima and learn less effective key-value mappings.
```
### Memory Perspective

```
Associative memoryâ€”the ability to map different entities or eventsâ€”is an inseparable component of learning in hu-
mans (Terry 2017) and so has motivated several recent studies to understand the state-of-the-art deep learning architectures
through its lens (Behrouz, Razaviyayn, et al. 2025; Behrouz, Zhong, et al. 2024; Ramsauer et al. 2021; Wang et al. 2025).
In this perspective, memory is defined as a neural update caused by an input; the more surprising the input is, the more
it affects the memory and so is memorable. Therefore, finding an effective â€œsurprise metricâ€ is a critical step towards
designing such memory modules. As earlier discussed by Behrouz, Razaviyayn, et al. (2025) and Behrouz, Zhong, et al.
(2024), almost all existing architectures use a surprise metric that updates the memory based on the current input. An
event (as a sequence of tokens), however, might not consistently be surprising through a long-period of time although it is
memorable. To overcome this issue, Behrouz, Zhong, et al. (2024) suggest breaking the surprise metric into two parts of
â€œmomentaryâ€ and â€œpastâ€ surprise, incorporating thecumulativesurprise of past inputs when updating the memory with
respect to the current input. This design, however, can miss thecontextby memorizing individual tokens. To this end, in
this work, we present a long-term neural memory module that measures the surprise of a local (or global) context window,
meaning that it learns how to memorize the (token) context at test time.
Through the paper, we use the terminology â€œTest Time Memorizationâ€ because the process involves storing and retrieving in-
```

formation strictly within the global context, without updating the modelâ€™s core learned parameters (i.e., outer-loop) or initial
states from pre-training. Typically, no persistent learning or skill acquisition carries over to new, independent global context
once the memory is cleared. Thus, we prefer the use of "test time memorization" over using "test time training".

### Contributions

In this paper, we aim to overcome the abovementioned limitationsâ€”i.e., (1) online nature, (2) limited memory capacity, and
(3) less expressive memory managementâ€”by designing a long-term neural memory module with high capacity and the
ability to memorize the context, instead of tokens. We further build upon these insights and present a family of strictly
more powerful Transformers. More specifically:

Better Understanding of Memory Capacity and its Bottleneck.To improve the limited memory capacity, we suggest
using higher-order feature mappings (e.g., polynomial feature kernels) on input tokens. We provide theoretical justifications
on why deeper memory modules and/or higher-order feature mapping can enhance memory capacityâ€”i.e., the maximum
number of linearly independent key-value associations the memory can perfectly map.

New Expressive Learning Rule.To overcome the online nature of recent recurrent models, this work presents a sliding
window update rule, called Omega rule, that optimizes and updates memory based on all past tokens in a given context
window, not just the last. This allows the model to better manage its fixed-size memory and memorize a local context
instead of individual tokens.

Strict Generalization of Transformers.Next, we show how our Omega rule formulation connects to global and local
softmax attentions (i.e., Sliding Window Attention - SWA) and present a new family of Transformer-like architectures,
calledDeepTransformersand its sliding window variants SWDT, that strictly generalize Transformers (Vaswani et al.
2017). We further present a novel baseline of Deep Linear Attention (DLA) to demonstrate the role of deep memory.

New Memory Modules with Better Memory Management.Building upon the above improvements, we present
OmegaNet, a new architecture using polynomial features on its keys and queries, while updating its memory based on
Omega and gradient descent. To further enhance memory management, we introduceAtlas, which leverages the popular
Muon optimizer (Jordan et al. 2024) for updating the internal memory. We show that bothOmegaNetandAtlascan
take advantage of parallelizable training algorithms, resulting in fast training without substantial overhead compared
to the online version (i.e., context window = 1). To the best of our knowledge,Atlasis the first parallelizable recurrent
architecture that optimizes the memory using the (approximation) of second-order information (i.e., has locally optimal
memory module).

Improvement on Diverse Downstream Tasks. Extensive experiments validate our model designs and proposed
techniques, including ablations of modern architectures. We evaluatedDeepTransformers,OmegaNet, andAtlason
diverse benchmarksâ€”language modeling, common-sense reasoning, recall-intensive, and needle-in-haystack tasksâ€”where
they outperformed modern linear RNNs, local attention (SWA), and Transformers. Furthermore, we studied the effects of
memory architecture, feature mapping, memory management algorithm (internal optimizer), and Omega rule on memory
module capacity and performance in long-context understanding tasks.

Proofs, additional experimental results, discussions on related work, and the details of experiments are in Appendix.

## 2 Preliminaries

In this section, we first discuss the notation that we use through the paper and then review the background concepts and
related work. Additional discussion on related studies are in Appendix A.

Notations.We letğ‘¥âˆˆRğ‘Ã—ğ‘‘inbe the input,Mğ‘¡be the state of memoryMat timeğ‘¡,Kbe the keys,Vbe the values,
andQbe the query matrices. We use bold lowercase letters with subscriptğ‘¡to refer to vectors correspond to timeğ‘¡
(i.e.,kğ‘¡,vğ‘¡, andqğ‘¡). Following Behrouz, Razaviyayn, et al. (2025), we useâ„“(Mğ‘¡;kğ‘¡,vğ‘¡)to refer to the attentional bias (i.e.,
the internal memory objective). Through the paper, we use simple MLPs withLMâ‰¥ 1 layers and residual connection
as the architecture of the memory moduleM(Â·). Notably, despite this choice, all of our model formulations are simply
adaptable to other memory architecture choices; e.g., linear matrix-valued memory (LM= 1 ). When it is needed, we


parameterized the memory module withğœ½M:={ğ‘Š 1 ,.. .,ğ‘ŠLM,.. .}, which at least includes the parameters of linear layers
in the MLP.

### 2.1 Backgrounds

Attention.Attention is a critical component of Transformers that acts as their associative memory (Behrouz, Razaviyayn,
et al. 2025; Bietti et al. 2023; Sun, Li, et al. 2024). Given inputğ‘¥âˆˆRğ‘Ã—ğ‘‘in, causal attention computes outputyâˆˆRğ‘Ã—ğ‘‘inover
input dependent key, value, and query matricesQ=ğ‘¥WQ,K=ğ‘¥WK,andV=ğ‘¥WVas:

```
yğ‘–=
```
##### âˆ‘ï¸ğ‘–

```
ğ‘—= 1
```
```
exp
```
##### 

```
qâŠ¤ğ‘–kğ‘—/
```
##### âˆš

```
ğ‘‘in
```
##### 

```
vğ‘—
Ãğ‘–
â„“= 1 exp
```
##### 

```
qğ‘–âŠ¤kâ„“/
```
##### âˆš

```
ğ‘‘in
```
##### =

##### 1

##### ğ‘ğ‘–

##### âˆ‘ï¸ğ‘–

```
ğ‘—= 1
```
```
exp
```
##### 

```
qâŠ¤ğ‘–kğ‘—/
```
##### âˆšï¸

```
ğ‘‘in
```
##### 

```
vğ‘—, (1)
```
whereWQ,WK,andWVâˆˆRğ‘‘inÃ—ğ‘‘inare learnable parameters, andğ‘ğ‘–=

##### Ãğ‘–

```
â„“= 1 exp
```
##### 

```
qâŠ¤ğ‘–kâ„“/
```
##### âˆš

```
ğ‘‘in
```
##### 

```
is the normalization term.
```
Despite Transformersâ€™ simple parallelizable training and effectiveness in recall-intensive tasks (Arora, Eyuboglu, Zhang,
et al. 2024), their generation process and long-context scaling are significant drawbacks, as attention requires at leastğ‘Ã—ğ‘‘
operations per token to calculate the output (see Equation 1). Therefore, in recent years, there have been an extensive
research effort to design alternative architectures. We divide and review these studies into two groups: (1) Linear shallow
memory recurrent models, (2) Deep memory modules:

(Linear) Recurrent Models.Linear RNNs have recently gained attention as efficient Transformer alternatives due to their
parallelizable, linear-time training and comparable performance (Peng, Alcaide, et al. 2023; Sun, Dong, et al. 2023). Early
modern RNN variants, often based on Hebbian (Hebb 2005) or Delta (Widrow et al. 1988) learning rules, compress data into
vector-valued or matrix-valued memory (Kacham et al. 2024b; Katharopoulos et al. 2020; Lim et al. 2024; Liu, Wang, et al.
2024; Schlag et al. 2021; Sun, Dong, et al. 2023). LetMğ‘¡âˆˆRğ‘‘Ã—ğ‘›be the memory (whereğ‘›= 1 yields vector-valued memory),
andk,vâˆˆRğ‘‘be the keys and values (projections of inputğ‘¥ğ‘¡âˆˆRğ‘‘)). A simple general formulation for such linear RNNs
is:

```
Mğ‘¡=ğ´ğ‘¡âˆ—Mğ‘¡âˆ’ 1 +vğ‘¡kâŠ¤ğ‘¡, (2)
```
whereâˆ—is an arbitrary associative operator andğ´ğ‘¡is a data-(in)dependent diagonal or low-rank plus identity matrix (Yang,
Wang, Zhang, et al. 2024). Despite the efficientlinearrecurrent nature of these models, their memory can overflow, particu-
larly with increasing context length. Although forget gates have recently significantly improved memory management in
these architectures (Peng, Zhang, et al. 2025; Sun, Dong, et al. 2023), their memoryâ€™s expressivity remains bounded by its
linear structure.

Deep Memory Module.To overcome the limited expressivity of memory and to enhance theeffectivecontext length
recurrent models, recent studies focus on a new line of architectures with deep memory modules (Behrouz, Razaviyayn,
et al. 2025; Behrouz, Zhong, et al. 2024; Irie et al. 2021; Sun, Li, et al. 2024). These architectures are built on the meta-learning
perspective, where the memory is a deep MLP architecture updated by gradient descent (with momentum). Recently,
Behrouz, Razaviyayn, et al. (2025) present a framework to accurately unifies popular sequence models as the instances of
test time memorization. That is, sequence models are associative memory modules that aim to learn the underlying mapping
between given keys and values by optimizing an internal memory objective, called attentional bias. This optimization
is based on an iterative optimization algorithms such as gradient descent. More formally, associative memory is defined
as:

Definition 1(Behrouz, Razaviyayn, et al. (2025)).Given a set of keysK âŠ†Rğ‘‘ğ‘˜and valuesV âŠ†Rğ‘‘ğ‘£, associative memory
is an mappingM:K â†’ V. Learning the associative memory is based on an objectiveL, calledAttentional Bias, that
determines the type of memory and its priorities:

```
Mâˆ—=arg min
M
```
##### L(M(K);V). (3)

Optimizing this objective using an iterative algorithm (e.g., gradient descent) results in the memory update rule. Thus, the
sequence model is a meta in-context learner with two optimization levels:


```
Figure 1: Comparison of learning to memorize (Left) individual tokens, and (Right) the context.
```
```
1.Inner Loop: Where parameters of the memory module are optimized (i.e.,ğœ½M={ğ‘Š 1 ,ğ‘Š 2 ,.. .,ğ‘ŠLM,...}). In the inner
optimization loop, all other parameters from the model are considered hyperparameters and are fixed andnot
optimized.
2.Outer Loop: Where all other parameters of the model are optimized, such as linear projections, MLP layers,
convolutions, etc.
```
Our terminology builds on this framework. Therefore, instead of full recurrent formulations, we describe models by their:
(1) memory architecture, (2) internal objective (i.e., attentional bias), and (3) memory learning algorithm (optimizer). In
most cases, models use matrix-valued memory with online gradient descent; for brevity in such instances, we refer to an
architecture solely by its internal memory objective. For additional discussions and examples, see Appendix B.

## 3 Learning to Memorize the Context at Test Time

Long-term associative memory, crucial for human learning (Terry 2017), has inspired many artificial neural architec-
tures (Behrouz, Razaviyayn, et al. 2025; Behrouz, Zhong, et al. 2024; He et al. 2024; Hopfield 1982; Krotov and Hopfield
2016; Ramsauer et al. 2021; Schmidhuber and Hochreiter 1997). While many such models use matrix- or vector-valued
memory to compress past data (Schlag et al. 2021; Von Oswald et al. 2023; Yang, Kautz, et al. 2024), recent studies advocate
for deep non-linear neural memory that encodes past abstractions into its parameters (Behrouz, Razaviyayn, et al. 2025;
Behrouz, Zhong, et al. 2024; Dalal et al. 2025; Sun, Li, et al. 2024). For long-context reasoning/understanding, however, these
long-term neural memory modules still require: (1) High capacityâ€”the maximum (key, value) pairs storable in parameters
(see Â§3.1); (2) A powerful internal memory objective (i.e.,attentional bias) to learn complex mapping between keys and
values (see Â§3.2); (3) Powerful memory management for better fixed-size memory management (see Â§3.2); and (4) An
efficient parallel training process for large-scale training on modern accelerators (see Â§3.3).

This section further discusses these challenges and presents Omega rule: an expressive memory update rule with direct
access to tokens in a local context window, which memorizes context rather than individual tokens.

### 3.1 Associative Memory with Super Linear Capacity

As previously discussed, an effective long-term memory module should store past data abstractions in its parameters.
However, with a fixed number of memory parameters, a key unanswered question remains: â€œwhat is the maximum number
of uncorrelated (key, value) pairs that a model can store?â€ To answer this, we start with the simplest case: matrix memory,
anâ„“ 2 regression loss as the attentional bias (i.e.,â„“(Mğ‘¡;kğ‘¡,vğ‘¡)=âˆ¥Mğ‘¡(kğ‘¡)âˆ’vğ‘¡âˆ¥^22 ), optimized by gradient descent:

Proposition 1(Capacity ofâ„“ 2 Attentional Bias).LetMbe a matrix-valued memory withğ‘‘ğ‘£Ã—ğ‘‘ğ‘˜parameters that optimizes
the internal objective ofâ„“(Mğ‘¡;kğ‘¡,vğ‘¡)=âˆ¥Mğ‘¡kğ‘¡âˆ’vğ‘¡âˆ¥^22 with gradient descent.Mcan store the mapping of at mostO(ğ‘‘ğ‘˜)
pairs of(kğ‘–,vğ‘–)with linearly independent keys.

The above proposition indicates that matrix-valued memory with delta update rule has sub-linear capacity with respect to
its number of parameters. This means that the number of independent patterns that can be stored in a fixed-size memory
with sizeğ‘€is strictly less thanğ‘Ã—ğ‘€, for someğ‘âˆˆR+. Recent recurrent models suggest using deep memory modules to


store the abstraction of the past into the parameters of a deep neural network (Behrouz, Razaviyayn, et al. 2025; Behrouz,
Zhong, et al. 2024; Irie et al. 2021; Sun, Li, et al. 2024). While these deep memory architectures can intuitively enhance the
expressive power in modeling complex underlying mapping patterns between keys and values, it is still unclear that if they
enhance the memory capacity.

Theorem 1(Effect of Deep Memory). LetM(Â·)be an MLP withLMâ‰¥ 2 layers,ğ‘‘ğ‘˜input dimension, andğ‘‘â„hidden

dimension. Then,M(Â·)can store the mapping of at leastO(ğ‘‘ğ‘˜ğ‘‘ğ‘£)and at mostO

##### 

##### ğ‘‘ğ‘˜ğ‘‘ğ‘£

##### ÃLM

```
ğ‘–= 1 min{ğ‘‘
```
```
(ğ‘—)
â„ }ğ‘—â‰¥ğ‘–ğ‘‘
```
```
(ğ‘—+ 1 )
â„
```
##### 

```
pairs of
```
(kğ‘–,vğ‘–)with linearly independent keys.

This theorem indicates that deep memory not only improves representational power but also further boosts network capacity,
with advantages growing with depth. However, the upper bound remains subquadratic in key and value dimensions,
raising the question if a long-term memory module can achieve super-linear capacity.

As stated earlier, the dimension ofkğ‘¡s is crucial for increasing memory capacity. Simply increasing all key and value
dimensions, however, significantly increase the number of parameters (O(ğ‘‘in)per each extra dimension) and memory usage,
particularly with long contexts. To address this, building on methods from Kacham et al. (2024a) and Krotov and Hopfield
(2016), we suggest using separable kernelsğœ(ğ‘¥,ğ‘¦)=ğœ™(ğ‘¥)âŠ¤ğœ™(ğ‘¦)for keys and queries. As an example of such kernels, we
focus on polynomial kernels of degree at mostğ‘to increase input dimensionality and thus network capacity. Givenğ‘âˆˆN,
letğœ™ğ‘(ğ‘¥)=[ğ‘¥ğ›½]|ğ›½|â‰¤ğ‘be a polynomial mapping ofğ‘¥with degree at mostğ‘. We redefine the associative memory module in
Definition 1 by replacing the inner objective ofL(M(K);V)withL(M(ğœ™(K));V). This polynomial mapping enhances
representational power by increasing the effective dimensionality of keys without additional parameter overhead for the
input projections. Next, we discuss their effect on memory capacity, even with a single matrix-valued memory:

Proposition 2(Memory Capacity with Polynomial Mapping). Letğœ™ğ‘(Â·)be a polynomial mapping with degree at most
ğ‘, andMbe a matrix-valued memory that optimizes the internal objective ofâ„“(Mğ‘¡;ğœ™ğ‘(kğ‘¡),vğ‘¡)=âˆ¥Mğ‘¡ğœ™ğ‘(kğ‘¡)âˆ’vğ‘¡âˆ¥^22 with
gradient descent.Mcan store the mapping of at mostO

##### 

##### ğ‘‘ğ‘˜ğ‘

##### 

pairs of(kğ‘–,vğ‘–)with linearly independent keys, whereğ‘‘ğ‘˜is the
dimension of keyskğ‘–.

Beyond the above intuition, polynomial kernels are further motivated by two perspectives: (1) Approximating Softmax
using Taylor series; and (2) Input feature gating. For the sake of clarity, we continue with linear memory and two popular
attentional biases i.e.,â„“(^1 )(Mğ‘¡;kğ‘¡,vğ‘¡)=âŸ¨Mğ‘¡kğ‘¡,vğ‘¡âŸ©andâ„“(^2 )(Mğ‘¡;kğ‘¡,vğ‘¡)=âˆ¥Mğ‘¡ğœ™(kğ‘¡)âˆ’vğ‘¡âˆ¥^22. The same process can be
applied on other attentional objectives and deep memory modules. Optimizing these objectives using gradient descent in
the inner loop results in the following recurrent formulas:

```
â„“(^1 )(Mğ‘¡;kğ‘¡,vğ‘¡) : Mğ‘¡=Mğ‘¡âˆ’ 1 +ğœ‚ğ‘¡vğ‘¡ğœ™(kğ‘¡)âŠ¤, (Hebbian Rule)
â„“(^2 )(Mğ‘¡;kğ‘¡,vğ‘¡) : Mğ‘¡=
```
##### 

```
Iâˆ’ğœ‚ğ‘¡ğœ™(kğ‘¡)ğœ™(kğ‘¡)âŠ¤
```
##### 

```
Mğ‘¡âˆ’ 1 +ğœ‚ğ‘¡vğ‘¡ğœ™(kğ‘¡)âŠ¤. (Delta Rule)
```
Kernel Attention Perspective for the Special Case of Hebbian Rule.The formulation for (Hebbian Rule) is equivalent
to kernel linear attentions (Arora, Eyuboglu, Zhang, et al. 2024; Hua et al. 2022; Kacham et al. 2024b; Kasai et al. 2021;
Katharopoulos et al. 2020; Wang et al. 2025). In this viewpoint, the role ofğœ™(.)is to approximateSoftmaxor more
accurately the exponential kernel. Since exponential kernel with normalization (i.e.,Softmax) is not separable, it results in
Transformersâ€™ quadratic time and memory complexity. However, Transformersâ€™ exponential feature map kernel (exp(qâŠ¤ğ‘–kğ‘—))
can be approximated using its Taylor series as:

```
exp
```
##### 

```
qâŠ¤ğ‘–kğ‘—
```
##### 

```
â‰ˆ 1 +qâŠ¤ğ‘–kğ‘—+
```
```
(qâŠ¤ğ‘–kğ‘—)^2
2!
```
##### +

```
(qâŠ¤ğ‘–kğ‘—)^3
3!
```
##### +... (4)

Our polynomial feature map extends this approximation to a more general case of:

```
exp
```
##### 

```
qâŠ¤ğ‘–kğ‘—
```
##### 

```
â‰ˆğœ™ğ‘(q)ğœ™(k)âŠ¤=ğ‘ 0 +ğ‘ 1 qğ‘–kâŠ¤ğ‘— +ğ‘ 2 (qğ‘–âŠ¤kğ‘—)^2 +ğ‘ 3 (qâŠ¤ğ‘–kğ‘—)^3 +Â·Â·Â·+ğ‘ğ‘(qâŠ¤ğ‘–kğ‘—)ğ‘, (5)
```
with learnable parametersğ‘ğ‘–âˆˆRinitialized atğ‘ğ‘–=ğ‘–^1 !, the polynomial kernel can be viewed as an expressive approximator
ofSoftmaxattention. This provides theoretical motivation for using polynomial kernels, especially when memory capacity
is limited; i.e., with (i) linear memory and (ii) Hebbian learning rule. This intuition, however, further generalizes to more
expressive cases using deep memory modules and more complex attentional biases (i.e., Eq. Delta Rule). That is,exp(Â·)
feature mapping has infinite dimension and provides a more powerful similarity measure of keys and queries (i.e.,qâŠ¤ğ‘–kğ‘—);


however, its computation with normalization can cause additional memory and time complexity to the model. Using
polynomial kernels in architectures with deep memory and complex attentional bias can further enhance performance
by approximating more powerful representations for keys-queries similarities (i.e.,qğ‘–âŠ¤kğ‘—). See Section 4 for additional
discussions on exponential kernels and Transformers.

Input Gating Interpretation.Another perspective that motivates the use of polynomial features is their more expressive
representational power in modeling complex functions compared to the simple case ofğœ™(ğ‘¥)=ğ‘¥. That is, the coefficients of
ğ‘ğ‘–s can be seen as input feature gating, in whichğ‘ğ‘–â†’ 0 means excluding the feature map of[ğ‘¥ğ‘—]|ğ‘—|=ğ‘–, andğ‘ğ‘–â†’ 1 means
retaining the corresponding feature. This is similar to the gating mechanisms of RNNs but on the input rather than the
memory. This gating mechanism clearly provides a more representational power as the model can learn to setğ‘ğ‘–â†’ 0 for
allğ‘–â‰  1 andğ‘ 1 â†’ 1 , resulting in the simple case ofğœ™(ğ‘¥)=ğ‘¥.

### 3.2 Long-term Memory with Context Memorization

As discussed earlier, one of the critical drawback of most existing recurrent models is their online nature, in which they
optimize the inner objective (attentional bias) with respect to only the current input while retaining the previous state of
the memory (Behrouz, Razaviyayn, et al. 2025; Liu, Wang, et al. 2024), i.e.,

```
min
M
```
```
â„“(M;kğ‘¡,vğ‘¡)+Retğ‘¡(M,Mğ‘¡âˆ’ 1 ), (6)
```
whereRet(Â·,Â·)is the retention gate. This online nature while making the optimization of the memory simpler and faster,
can cause sub-optimal memorization of the context as memory is greedily memorize individual tokens. In a more general
case, however, one can optimize the memory at each time stamp with respect to the entire context (input sequence),
i.e.,

```
min
M
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–= 1
```
```
â„“(M;kğ‘–;vğ‘–). (7)
```
This strict global formulation generally presents two critical limitations: (1) Efficiency: One of the important advantages of
recurrent architectures is their efficiency at longer context in both training and inference. Optimizing the memory with
respect to all the past tokens (entire context), however, (i) causes additional optimization constraints at each memory
update step, resulting in inefficiency at extremely large sequences, and (ii) requires caching the past keys and values at
the test time, increasing the memory consumption; (2) Context Pruning: In large context tasks optimizing with all past
tokens can cause sub-optimal performance mainly due to the context change (or irrelevant context) in the middle of the
input sequence. This observation has resulted to design architectures with retention (forget) gate, enabling models to erase
memory when past context is no longer needed (Behrouz, Razaviyayn, et al. 2025; Behrouz, Zhong, et al. 2024; Peng, Zhang,
et al. 2025; Sun, Dong, et al. 2023; Yang, Wang, Shen, et al. 2024).

To address these limitations, we present a sliding window recurrent model that optimizes its attentional bias w.r.t. a
window of past tokens. For a memory moduleM(Â·)and window lengthğ‘â‰¥ 1 , we optimize the memory internal objective
as:

```
min
M
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğ›¾ğ‘–(ğ‘¡)â„“(M;kğ‘–,vğ‘–), (8)
```
whereâ„“(M;kğ‘–,vğ‘–)measures the predicted mapping for(kğ‘–,vğ‘–)pair andğ›¾ğ‘–(ğ‘¡)is the decay term for the effect ofğ‘–-th token in
the optimization process. Building upon this formulation, we present Omega rule, which is strictly more powerful than the
popular Delta learning rule (Schlag et al. 2021; Widrow et al. 1988):

```
Omega Rule: Letkğ‘–âˆˆRğ‘‘ğ‘˜andvğ‘–âˆˆRğ‘‘ğ‘£be the input keys and values, andM(Â·)be a neural architecture that
serves as the memory module. Given a local context length ofğ‘âˆˆNâ‰¥ 1 , the updating the memory moduleMusing
Omega learning rule is defined as optimizing the following loss function with gradient descent:
```
```
min
M
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğ›¾ğ‘–(ğ‘¡)âˆ¥M(kğ‘–)âˆ’vğ‘–âˆ¥^22 (9)
```

```
Figure 2: The illustration of tokens dependencies in SWA andAtlasorOmegaNetwith different context length.
```
Following Behrouz, Razaviyayn, et al. 2025, this update rule can be extended toğ‘-Omega rule (or other variants) by
replacingâ„“ 2 (Â·)withâ„“ğ‘(Â·). In the extreme cases of (1)ğ‘= 1 : the update rule becomes online (Delta rule); and (2)ğ‘=âˆor

context length: the update becomes global optimization w.r.t. all past tokens. In this formulation, parametersğ›¾ğ‘–(ğ‘¡)âˆˆ [ 0 , 1 ]

act as hard (direct) gates for the past tokens. That is,ğ›¾ğ‘–(ğ‘¡)â†’ 0 means that the model directly prunes the optimization of

ğ‘–-th token in the local context, whileğ›¾ğ‘–(ğ‘¡)â†’ 1 means fully incorporating the optimization of memory forğ‘–-th token in the

local context. In our design, we use input-dependent parameters forğ›¾ğ‘–(ğ‘¡), providing in-context pruning ability. Note that,
the design of sliding window recurrence allows such flexibility as for each token we need a constant number of gates;

i.e.,{ğ›¾ğ‘–(ğ‘¡)}ğ‘ğ‘–= 1. Using input-dependent gates for the global optimization (Equation 7), however, can result in significant
parameter increase and memory usage, diminishing the advantages of recurrent models.

OmegaNet.We now presentOmegaNet, a novel sequence model that updates its memory using Omega rule. To enhance
the memory capacity ofOmegaNet, we use polynomial kernels onks andqs. Accordingly, optimizing the objective in
Equation 9, results in an update rule ofOmegaNetas:

##### Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’âˆ‡

##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğ›¾ğ‘–(ğ‘¡)âˆ¥M(ğœ™(kğ‘–))âˆ’vğ‘–âˆ¥^22
| {z }
Surprise of the context
```
##### , (10)

or in the spacial case of linear memory:

##### Mğ‘¡=

```
diag(ğ›¼ğ‘¡)âˆ’
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğ›¾ğ‘–(ğ‘¡)ğœ™(kğ‘–)ğœ™(kğ‘–)âŠ¤
```
##### !

##### Mğ‘¡âˆ’ 1 âˆ’

##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğ›¾ğ‘–(ğ‘¡)vğ‘–ğœ™(kğ‘–)âŠ¤. (11)
```
From the memory perspective, Omega rule (OmegaNet) does not measure the surprise of a token, but the surprise of a
local context based on the context-aware combination of individual tokens within the context.

Beyond Gradient Descent.The concept of Omega rule and â€œtest time memorization of contextâ€ can simply be extended
to optimizing the objective in Equation 9 with any arbitrary optimizer, even beyond simple gradient descent. We use two
extreme cases forğ‘as the illustrations. In the first case, we letğ‘= 1 ,ğ›¾ğ‘–(ğ‘¡)= 1 , and use gradient descent with momentum as
the optimizers, resulting in the following update rule:

```
Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 +Sğ‘¡ (12)
Sğ‘¡=ğœƒğ‘¡Sğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡). (13)
```
This update rule is equivalent to the long-term neural memory in Titans (Behrouz, Zhong, et al. 2024). In the second case,
using a linear memoryM, lettingğ›¾ğ‘–(ğ‘¡)= 1 , andğ‘be equal to the context length, the memory update process is equivalent
to optimizing the (regularized) least-squares problem:

```
Mğ‘¡=min
M
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–= 1
```
```
âˆ¥Mkğ‘–âˆ’vğ‘–âˆ¥^22. (14)
```

Von Oswald et al. (2023) suggest directly optimizing the above objective and use Sherman-Morrison formula (Sherman
et al. 1950) to recursively calculate the inverse term in the optimal solution. Despite the optimality of memory, such direct
solutions comes with the cost of non-parallelizable training and also are limited to only the linear matrix-valued memory
setup. Furthermore, as discussed earlier, the global nature without any direct hard gating terms (i.e.,ğ›¾ğ‘–(ğ‘¡)s) can force the
model tonotprune the context, damaging the performance in longer sequences.

### 3.3 Parallelizing Omega Rule

While Omega rule provides a more general and expressive formulation for the design of memory modules than Hebbian or
Delta learning rules, its applicability to large-scale models relies on its efficiency in training. To this end, we discuss a fast
parallelizable training algorithms that does not add any significant computational overhead with the online counterpart
version (i.e.,ğ‘= 1 ). A naive implementation requires materializingğ‘gradientsâˆ‡â„“ âˆˆRğ‘‘inÃ—ğ‘‘in, which can result in a
significantly higher memory footprint and I/O cost whenğ‘‘inis large. Also, to fully utilize hardware accelerators such
as TPUs and GPUs, it is important to tensorize computations and maximize the use ofmatmuloperations. Motivated
by recent work (Behrouz, Zhong, et al. 2024; Sun, Li, et al. 2024), we propose a simple sliding window masking strategy
that supports efficient parallel training while avoiding substantial memory overhead. Specifically, we partition the input
sequence with lengthğ¿into chunks of sizeğ‘â‰¥ 1 , each of which is represented bySğ‘–={x(ğ‘–âˆ’ 1 )ğ‘+ 1 ,.. .,xğ‘–ğ‘}. Then for each
chunk, we calculate the gradients with respect to the last state of the previous chunk. For the sake of clarity, we first

assumeğ›¾ğ‘–(ğ‘¡)=ğœ‚ğ‘¡for all positions in the sequence. When the chunk size isğ‘= 1 , the update rule is:

##### Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡

##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
âˆ‡â„“(Mğ‘¡âˆ’ 1 ;kğ‘–,vğ‘–), (15)
```
whereMğ‘¡is the model state at stepğ‘¡,ğ›¼ğ‘¡andğœ‚ğ‘¡are the weight decay and learning rate parameters respectively, and(kğ‘–,vğ‘–)
denote the input pair at positionğ‘–. In practice, we strike a balance between the fully recurrent form and the fully parallel
form by dividing the sequence into smaller chunks. Within each chunk (intra-chunk), we apply parallel computation, while
across chunks (inter-chunk), we adopt a recurrent computation scheme. We now defineğ‘¡â€²=ğ‘¡âˆ’mod(ğ‘¡,ğ‘). That is, for time
stepsğ‘¡such thatğ‘¡â€²â‰¤ğ‘¡<ğ‘¡â€²+ğ‘, the update rule within each chunk becomes:

##### Mğ‘¡=ğ›¼ğ‘¡...ğ›¼ğ‘¡â€²Mğ‘¡â€²âˆ’

##### âˆ‘ï¸ğ‘¡

```
ğ‘›=ğ‘¡â€²
```
##### ğ›¼ğ‘¡...ğ›¼ğ‘¡â€²

##### ğ›¼ğ‘›...ğ›¼ğ‘¡â€²

##### ğœ‚ğ‘›

##### âˆ‘ï¸ğ‘›

```
ğ‘–=ğ‘›âˆ’ğ‘+ 1
```
```
âˆ‡â„“(Mğ‘¡â€²;kğ‘–,vğ‘–)
```
```
| {z }
ğºğ‘¡
```
##### (16)

In our implementation, forğºğ‘¡, we follow the same gradient computation approach as described in Titans (Behrouz, Zhong,
et al. 2024) but additionally apply a sliding window maskğ‘€ğ‘ during the broadcasting operation (e.g., usingeinsum). When
ğ‘= 1 , the sliding window maskğ‘€ğ‘ reduces to the identity matrix. Forğ‘> 1 ,ğ‘€ğ‘ is an identity matrix except that theğ‘âˆ’ 1
positions immediately preceding each diagonal entry are also set to 1. This allows gradient contributions from a window
of sizeğ‘, enabling efficient computation without materializing all gradients inside the chunk.

## 4 DeepTransformers: Transformers with Deep Memory

Recent studies have extensively discussed Transformer architectures through the lens of associative memory (Behrouz,
Razaviyayn, et al. 2025; Sun, Li, et al. 2024; Wang et al. 2025). Accordingly, it is natural to ask how our discussions of
memory capacity as well as Omega rule can affect Transformers. In this section, we discuss how our formulation of
Omega rule is connected to Transformers and their sliding window counterparts (i.e., SWA). We then further provide two
extensions to Transformers, each of which is a strict generalization of Transformers.

### 4.1 Online and Local Context Optimization of Memory

Connection to Sliding Window Attention.Softmaxattention block can also be reformulated as a non-parametric
solution to theâ„“ 2 (Â·)regression with Nadaraya-Watson estimators (Fan 2018; Zhang et al. 2022):

```
Mâˆ—=arg min
M
```
##### âˆ‘ï¸ğ¿

```
ğ‘–= 1
```
```
s(kğ‘–,q)âˆ¥vğ‘–âˆ’Mâˆ¥^22 =
```
##### âˆ‘ï¸ğ¿

```
ğ‘–= 1
```
```
s(kğ‘–,q)
Ãğ¿
ğ‘—= 1 s(kğ‘—,q)
```
```
vğ‘–, (17)
```

whereğ¿is the sequence length. While this formulation optimizes the memoryMwith respect to the entire sequence
length, one can limit the optimization process to the pastğ‘tokens, resulting in:

```
Mâˆ—=arg min
M
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
s(kğ‘–,qğ‘–)âˆ¥vğ‘–âˆ’Mâˆ¥^22 =
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
s(kğ‘–,q)
Ãğ‘¡
ğ‘—=ğ‘¡âˆ’ğ‘+ 1 s(kğ‘—,q)
```
```
vğ‘–, (18)
```
which is equivalent to the sliding window attention (SWA). This connection provides an important insight on the difference
of attention and recurrent models: Not only attention is a non-parametric solution (contrary to the parametric nature
of recurrent models), it globally optimizes its internal objective (attentional bias), while most recent modern recurrent
models are online learners (Behrouz, Razaviyayn, et al. 2025; Peng, Zhang, et al. 2025; Sun, Li, et al. 2024; Yang, Kautz, et al.
2024)^1. Our formulations of sliding window RNN and Omega rule fill this gap by optimizing the memory with respect to a
context window of past tokens based on parametric methods, effectively memorizing the context instead of individual
tokens.

Deep Linear Attention.As a novel baseline, we present Deep (Gated) Linear Attention (DLA) that replaces a matrix-valued
memory in (gated) linear attention (Katharopoulos et al. 2020; Yang, Wang, Shen, et al. 2024) with a deep neural network
(e.g.,ğ‘˜-layer MLP). As discussed earlier in (Hebbian Rule), using dot product similarity as the internal attentional bias
results in linear attention. Thus, leveraging recent deep memory modules (Behrouz, Razaviyayn, et al. 2025; Behrouz,
Zhong, et al. 2024; Sun, Li, et al. 2024), we optimize the memory using gradient descent with dot product attentional
bias:

```
Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡â„“(Mğ‘¡âˆ’ 1 ;ğœ™(kğ‘¡),vğ‘¡), (19)
```
whereâ„“(Mğ‘¡âˆ’ 1 ;ğœ™(kğ‘¡),vğ‘¡) =âŸ¨Mğ‘¡âˆ’ 1 (ğœ™(kğ‘¡)),vğ‘¡âŸ©andğœ™(Â·)is a polynomial kernel. The training of DLA can simply be
parallelized using the hybrid of linear and non-linear chunk-wise training, the same as Behrouz, Zhong, et al. (2024) and
Sun, Li, et al. (2024) and our discussion in Section 3.3.

Sliding Window Linear Attention.Building upon the above intuition and the connection of our formulation to SWA,
we present Sliding Window Linear Attention (SWLA) block. Following the formulation of linear attention in associative
memory perspective (Behrouz, Razaviyayn, et al. 2025), we use dot product similarity (i.e.,â„“(Mğ‘¡;kğ‘–,vğ‘–)=âŸ¨Mğ‘¡(kğ‘–),vğ‘–âŸ©) as
the attentional bias and optimize the loss function using gradient descent. For the sake of clarity, we use a linear memory
here to derive the closed form:

##### Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡

##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
â„“(Mğ‘¡âˆ’ 1 ;ğœ™(kğ‘–),vğ‘–)=Mğ‘¡âˆ’ 1 +
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğ›¾ğ‘–(ğ‘¡)vğ‘–ğœ™(kğ‘–)âŠ¤ (20)
```
In the online case (ğ‘= 1 ) andğœ™(Â·)=(Â·), this recurrence is the same as linear attention (Katharopoulos et al. 2020).

### 4.2 Memory Capacity and Exponential Kernels

We first recall the formulation ofsoftmaxattention in Transformers (i.e., Equation 1):

```
yğ‘–=
```
##### 1

##### Ãğ‘–

```
â„“= 1 exp
```
##### 

```
qâŠ¤ğ‘–kâ„“/
```
##### âˆš

```
ğ‘‘in
```
##### 

##### âˆ‘ï¸ğ‘–

```
ğ‘—= 1
```
```
exp
```
##### 

```
qğ‘–âŠ¤kğ‘—/
```
##### âˆšï¸

```
ğ‘‘in
```
##### 

```
vğ‘—, (21)
```
which itsexp(Â·)kernel is not separable and so cannot be written as a recurrence. Following the discussion in Kacham et al.
(2024b), one can seeexp(Â·)kernel (compared to polynomial kernelğœ™ğ‘(Â·)) as a feature map that maps the input into an

(^1) Two of the exceptions are Titans (Behrouz, Zhong, et al. 2024) and Mesa-layer (Von Oswald et al. 2023), where Mesa-layer optimizes the memory with
respect to all past tokens (comes with the cost of slow training), and Titans optimizes the memory with respect to all past tokens but with an implicit
decay term (i.e., the result of the momentum) for each past token, maintaining parallelizability.


infinite dimension. That is, we define:

##### ğœ™âˆ—(ğ‘¥)=

##### Â© Â­ Â­ Â­ Â­ Â­ Â­ Â­ Â­ Â«

##### 1

```
âˆšğ‘¥
1
ğ‘¥âˆšâŠ—^2
2!
ğ‘¥âˆšâŠ—^3
3!
..
.
```
##### Âª Â® Â® Â® Â® Â® Â® Â® Â® Â¬

##### , ğœ™ğ‘(ğ‘¥)=ğ‘¥âŠ—ğ‘, (22)

whereğ‘¥âŠ—ğ‘=ğ‘¥âŠ—ğ‘¥âŠ—(ğ‘âˆ’^1 )is a â€œself-tensoringâ€ operator with Kronecker product (Kacham et al. 2024b) and so:

```
exp(qâŠ¤ğ‘¡kğ‘¡)=ğœ™âˆ—(qğ‘¡)âŠ¤ğœ™âˆ—(kğ‘¡). (23)
```
Based on the above kernel, we can reformulate the attention (see Equation 21) as: (we remove 1 /

##### âˆš

ğ‘‘interm for the sake of
simplicity)

```
yğ‘–=
```
##### 1

##### Ãğ‘–

```
â„“= 1 exp
```
##### 

```
qâŠ¤ğ‘–kâ„“/
```
##### âˆš

```
ğ‘‘in
```
##### 

##### âˆ‘ï¸ğ‘–

```
ğ‘—= 1
```
```
vğ‘—ğœ™âˆ—(kğ‘—)âŠ¤ğœ™âˆ—(qğ‘–)=
```
##### 1

##### Ãğ‘–

```
â„“= 1 exp
```
##### 

```
qâŠ¤ğ‘–kâ„“/
```
##### âˆš

```
ğ‘‘in
```
##### 

(^) ğ‘–
âˆ‘ï¸
ğ‘—= 1
ğœ™âˆ—(vğ‘—kğ‘—)âŠ¤

##### !

```
ğœ™âˆ—(qğ‘–)=Mğ‘–ğœ™âˆ—(qğ‘–), (24)
```
This formulation, provides another important insight on the differences of attention and (kernel) recurrent models:Softmax
attention as an associative memory has an unbounded memory and so can better memorize larger context into its parameters.
Building upon this insight, we presentDeepTransformersby replacing polynomial kernel withğœ™âˆ—(Â·)kernel in Deep
Linear Attention formulation (Equation 19), resulting in unnormalized formulation of:

```
Mğ‘¡=Mğ‘¡âˆ’ 1 âˆ’âˆ‡âŸ¨Mğ‘¡âˆ’ 1 (ğœ™âˆ—(kğ‘¡)),vğ‘¡âŸ©. (25)
```
In the special case of linear memory, we can derive the closed form for the above formulation as:

```
Mğ‘¡=Mğ‘¡âˆ’ 1 âˆ’âˆ‡âŸ¨Mğ‘¡âˆ’ 1 ğœ™âˆ—(kğ‘¡),vğ‘¡âŸ©=Mğ‘¡âˆ’ 1 +vğ‘¡ğœ™âˆ—(kğ‘¡)âŠ¤=
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–= 1
```
```
vğ‘–ğœ™âˆ—(kğ‘–)âŠ¤ â‡’ yğ‘¡=Mğ‘¡ğœ™âˆ—(qğ‘¡)=
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–= 1
```
```
vğ‘–exp(qğ‘–âŠ¤kğ‘–),(26)
```
which matches the output of the unnormalized Transformers. Therefore,DeepTransformersare strict generalizations of
Transformers withsoftmaxattention (Vaswani et al. 2017).

### 4.3 Deep Omega Transformer (Dot): Transformers with Omega learning rule

Our above formulation ofDeepTransformersis based on the (Hebbian Rule), which is also used in original Transformers.
However, as discussed earlier, using more powerful memory management and learning rules in associative memory
modules can further enhance their performance. To this end, we extend the above formulation by replacing the Hebbian
rule with our Omega learning rule, resulting in an unnormalized formulation of Deep Omega Transformers (Dot):

##### Mğ‘¡=Mğ‘¡âˆ’ 1 âˆ’âˆ‡

##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğ›¾ğ‘–(ğ‘¡)âˆ¥M(ğœ™âˆ—(kğ‘–))âˆ’vğ‘–âˆ¥^22. (27)
```
We now discuss special instances ofDotto provide further intuition on its generalized formulation.

Linear Memory.This setup results in the following unnormalized formulation:

##### Mğ‘¡=

##### Iâˆ’

##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğ›¾ğ‘–(ğ‘¡)ğœ™âˆ—(kğ‘–)ğœ™âˆ—(kğ‘–)âŠ¤
```
##### !

##### Mğ‘¡âˆ’ 1 âˆ’

##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğ›¾ğ‘–(ğ‘¡)vğ‘–ğœ™âˆ—(kğ‘–)âŠ¤ (28)
```
```
â‡’yğ‘¡=Mğ‘¡ğœ™âˆ—(qğ‘¡)=
Iâˆ’
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğ›¾ğ‘–(ğ‘¡)ğœ™âˆ—(kğ‘–)ğœ™âˆ—(kğ‘–)âŠ¤
```
##### !

```
Mğ‘¡âˆ’ 1 ğœ™âˆ—(qğ‘¡) âˆ’
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğ›¾ğ‘–(ğ‘¡)vğ‘–exp(qğ‘¡âŠ¤kğ‘–). (29)
```

Online Case withğ‘= 1 .We now letğ‘= 1 :

##### Mğ‘¡=

##### 

```
Iâˆ’ğœ‚ğ‘¡ğœ™âˆ—(kğ‘¡)ğœ™âˆ—(kğ‘¡)âŠ¤
```
##### 

```
Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡vğ‘¡ğœ™âˆ—(kğ‘¡)âŠ¤ (30)
â‡’yğ‘¡=Mğ‘¡ğœ™âˆ—(qğ‘¡)=
```
##### 

```
Iâˆ’ğœ‚ğ‘¡ğœ™âˆ—(kğ‘¡)exp(qâŠ¤ğ‘¡kğ‘¡)
```
##### 

```
Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡vğ‘¡exp(qğ‘¡âŠ¤kğ‘¡). (31)
```
The above (unnormalized) formulation can be seen as the generalization of Transformers with Delta Rule. Therefore, due
to the unbounded memory,Dotnot only appends the new keys and values (similar to original Transformers), but it also
replaces the new value with its predicted value from the previous state.

## 5 Atlas: A Locally Optimal Memory with High Capacity

Although the design of Omega rule allows the model to memorize the context instead of individual tokens and also the use
of polynomial (or exponential) feature mapping increases memory capacity, the memory management (i.e., optimization of
mappings between keys and values) is still limited to a simple gradient descent. This choice of optimizer can lead the model
to a low-quality solution at a local optima, damaging the performance of the model in longer contexts. To overcome this
issue, we suggest using Muon optimizer (Jordan et al. 2024) (with weight decay) that not only approximates second-order
information, but it also mostly leverages matrix multiplication and can be parallelized across the sequence. Accordingly,
the use of Muon for optimizing the internal objective in Equation 9, results in the following update rule:

```
Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡NewtonShulz-ğ‘˜(Sğ‘¡), (32)
```
##### Sğ‘¡=ğœƒğ‘¡Sğ‘¡âˆ’ 1 +âˆ‡

##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğ›¾ğ‘–(ğ‘¡)âˆ¥M(ğœ™âˆ—(kğ‘–))âˆ’vğ‘–âˆ¥^22 , (33)
```
whereğ‘is the local context length andğ‘˜is the number steps forNewtonShulzoperations. For the additional discussion on
the algorithm and this operation we refer the reader to Jordan et al. (2024). Following the literature on Muon optimizer, we
know that whenğ‘˜â†’âˆ, thenNewtonShulz-ğ‘˜(Sğ‘¡)converges to the nearest semi-orthogonal matrix to the momentum
termSğ‘¡and so approximate second-order information with a lower error. Therefore, interestingly, parameterğ‘˜can be
considered as an internal test-time compute parameter inAtlas, where using more steps can potentially result in better
memorization.

### 5.1 Parallel Training

In this section, we discussed how the training process ofAtlascan be parallelized. For the sake of clarity, we assume
ğ‘= 1. Generalizing the process to arbitrary value forğ‘follows the procedure in Section 3.3. We use the same process as
we discussed in Section 3.3 and so chunk the sequence and compute all the gradients with respect to the last state of the
previous chunk. Accordingly, using the recurrence ofAtlaswith momentum but without , we have:

```
Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 +Sğ‘¡ (34)
Sğ‘¡=ğœƒğ‘¡Sğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡â„“(Mğ‘¡â€²;kğ‘¡,vğ‘¡). (35)
```
Sinceğ‘¡â€²is the last state of the previous chunk, we can calculate all the gradients before hand and so we letğ‘¢ğ‘¡ =
âˆ‡â„“(Mğ‘¡â€²;kğ‘¡,vğ‘¡). Therefore, we have:

```
Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 +Sğ‘¡ (36)
Sğ‘¡=ğœƒğ‘¡Sğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡ğ‘¢ğ‘¡. (37)
```
Now by expanding the second recurrence, we have:

```
Sğ‘¡=ğœƒğ‘¡Sğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡â„“(Mğ‘¡â€²;kğ‘¡,vğ‘¡)
| {z }
ğ‘¢ğ‘¡
```
##### , (38)

##### â‡’Sğ‘¡=ğœƒğ‘¡...ğœƒ 1

```
|{z}
ğ›½ğ‘¡
```
##### S 0 âˆ’

##### âˆ‘ï¸ğ‘¡

```
ğ‘–= 1
```
##### ğœƒğ‘¡...ğœƒ 1

##### ğœƒğ‘–...ğœƒ 1

##### ğœ‚ğ‘–ğ‘¢ğ‘–=ğ›½ğ‘¡S 0 âˆ’Î˜âŠ™ğ¸âŠ™ğº, (39)


```
Figure 3: Visualization of theAtlasâ€™s (and our other variantsâ€™) architecture, and its hybrid counterpart with SWA.
```
```
whereğºis the gradient matrix,ğ¸andÎ˜are diagonal matrices with valueğœƒandğœ‚, andâŠ™is broadcasting.
```
The main advantage of the above formulation (chunk wise recurrence) is that the recurrence of momentum is independent
of the state of memory. That is, we can calculate all the momentum terms in the beginning of the chunk using the above
formulation. Now in the Muon case, we want to use Newton-Schulz algorithm on the momentum terms, which results
in:

```
Sğ‘¡â€²â†Newton-Schulz5(Sğ‘¡), (40)
Mğ‘¡=Mğ‘¡âˆ’ 1 +Sğ‘¡â€². (41)
```
Since the calculation of allSğ‘¡s can be done in parallel, the calculation ofNewton-Schulz5(Â·)can also be done in paral-
lel.

Architectural Backbone.As for the architectural backbone, we follow the recent modern recurrent models (Allen-Zhu
2025; Arora, Eyuboglu, Zhang, et al. 2024; Behrouz, Zhong, et al. 2024; Yang, Wang, Zhang, et al. 2024) and use linear layers
to project keys, values, and queries, followed by short convolution layers with size 4. We apply normalization on keys and
queries to stabilize the training. We also follow Behrouz, Zhong, et al. (2024) and use two hybrid variants of MAL and
MAG for ourAtlasmodel. The architectures are illustrated in Figure 3. For models with deep memory architectures we
use 2-layer MLP with residual connections:

```
M(Â·)=(Â·)+ğ‘Š 1 ğœ(ğ‘Š 2 (Â·)). (42)
```
We further extend this memory architecture, which is commonly used in recent studies (Behrouz, Razaviyayn, et al. 2025;
Behrouz, Zhong, et al. 2024; Irie et al. 2021), to gated MLP layer as:

```
M(Â·)=(Â·)+ğ‘Š 1 (ğœ(ğ‘Š 2 (Â·))âŠ—ğ‘Š 3 (Â·)), (43)
```
whereğ‘Š 1 ,ğ‘Š 2 ,ğ‘Š 3 are linear learnable matrices. We refer toAtlaswith the above memory architecture asAtlas++.

## 6 Experiments

Next, we evaluate the performance ofAtlas,OmegaNet,DeepTransformers, andDotin language modeling, common-
sense reasoning, needle in haystack, and in-context recall tasks. Although we also discussed several other variants, such as
SWLA, in our experiments we focus on the above models so in addition to comparison with state-of-the-art models, we
also answer the following questions:

```
1.Is deep memory effective forsoftmaxattention? (see Table 2 â€” comparison of Transformer++ andDeepTrans-
formers)
```

```
2.Does the use of Omega improve the performancesoftmaxattention? (see Table 2 â€” comparison of Transformer++,
DeepTransformers, andDot)
3.Does the Omega rule provide more expressive memory update? (see Table 2 and Table 6 â€” the performance of
OmegaNet, andAtlas)
```
4. Is locally optimal memory update effective? (see Table 2 and Table 6 â€” comparison ofOmegaNet, andAtlas)
5. Is non-linear feature mapping effective? (see Table 6)
6. Can the proposed improvements close the gap with Transformers in in-context recall tasks? (see Table 5)
7. What is the effect of the internal optimizer on the memory? (see Figure 6)

Setup.We train our models with training context window of size 4K using FineWeb dataset (Penedo et al. 2024). We use
model size of 340M, 400M, 790M, and 1.3B parameters and train them on 15B, 15B, 30B, and 100B tokens sampled from
the dataset. Baseline results are reported by Behrouz, Razaviyayn, et al. (2025), Behrouz, Zhong, et al. (2024), and Yang,
Kautz, et al. (2024). Perplexity is measured on held-out validation data. As for the downstream tasks, we evaluate trained
models on Wikitext (Merity et al. 2017), LMB (Paperno et al. 2016), PIQA (Bisk et al. 2020), HellaSwag (Zellers et al. 2019),
WinoGrande (Sakaguchi et al. 2021), ARC-easy (ARC-e) and ARC-challenge (ARC-c) (Clark, Cowhey, et al. 2018), SIQA (Sap
et al. 2019), and BoolQ (Clark, Lee, et al. 2019). Additional details about the experimental setups and other used datasets
are in Appendix E.

### 6.1 Language Modeling and Common-Sense Reasoning

The results forAtlas, andOmegaNetas well as their corresponding baselines of SWDT, DLA,DeepTransformers,
andDotwith the size of 760M and 1.3B are reported in Table 2. (see Appendix F for the results of small scale). Among
non-hybrid models, including Transformer++, ourAtlas, andOmegaNetachieve the best performance in both perplexity
and accuracy measures. We attribute this performance to their ability to memorize the context rather than individual tokens.
ComparingOmegaNetwith Titans, that also uses the same momentary objective (i.e.,â„“ 2 loss), but with context window
of 1, we can observe the effectiveness of having non-online learning rule. On the other hand, our models, alone without
any attention, can outperform hybrid models, while their hybrid variant of MAG further improve their performance. This
performance gain is also related to the use of polynomial kernels that enhance the memory capacity of the model. See
Table 6 for a more controlled study on the effect of different components.

Comparing Transformer++ with our more generalized Transformers (i.e.,DeepTransformers, andDot) we observe a
consistent performance improvement. We attribute this performance to their deep memory, which makes them more
powerful to model the dependencies of tokens. ComparingDotwithDeepTransformers, we can see the advantage of
Omega rule, which helps the model to better manage its memory.

Figure 4: Performance ofAtlasand baselines on BABILong
benchmark.Atlassurpasses Titans performance and effec-
tively scale to 10M context length in this task.

```
Figure 5: The effect of local context length (i.e.ğ‘) on the
performance ofOmegaNetwith different global context
length.
```

Table 2: Performance ofAtlasand baselines on language modeling and common-sense reasoning tasks. Hybrid models
are marked withâˆ—. The best results are highlighted highlighted.

```
Model Wiki. LMB. LMB. PIQA Hella. Wino. ARC-e ARC-c SIQA BoolQ Avg.
pplâ†“ pplâ†“ accâ†‘ accâ†‘ acc_nâ†‘ accâ†‘ accâ†‘ acc_nâ†‘ accâ†‘ accâ†‘ â†‘
760M params / 30B tokens
Transformer++ 25.21 27.64 35.8 66.9 42.2 51.9 60.4 32.5 39.5 60.4 48.
DeepTransformers(ours) 20.32 20.67 36.9 68.4 49.8 52.8 65.7 34.9 40.2 61.8 51.
Dot(ours) 19.96 20.15 39.0 69.1 50.7 53.1 66.2 37.0 40.3 63.7 52.
RetNet 26.08 24.45 34.5 67.2 41.6 52.1 63.2 32.8 38.4 57.9 48.
DeltaNet 24.37 24.60 37.1 66.9 42.0 50.7 64.9 31.4 39.9 59.0 48.
TTT 24.17 23.51 34.7 67.3 43.9 51.0 64.5 33.8 40.2 59.6 47.
Gated DeltaNet 21.18 22.09 35.5 68.0 44.9 50.7 66.9 33.1 39.2 59.1 49.
Sambaâˆ— 20.63 22.71 39.7 69.2 47.4 52.0 66.9 33.2 39.0 61.2 51.
Gated DeltaNet-H2âˆ— 19.88 20.83 39.2 69.0 48.2 52.6 67.0 35.5 39.4 61.1 51.
Titans (LMM) 20.04 21.96 37.4 69.3 48.5 52.3 66.3 35.8 40.1 62.8 51.
Memora 22.28 22.31 38.2 67.8 49.3 53.3 63.6 36.1 40.9 63.0 51.
SWDT (ours) 19.89 21.52 36.2 68.3 45.2 53.0 65.4 34.2 39.5 59.5 50.
DLA (ours) 23.12 22.09 36.1 68.0 47.9 52.7 65.8 34.6 39.1 59.6 50.
OmegaNet(ours) 19.16 20.14 38.7 69.8 50.0 53.3 67.8 36.8 39.6 64.4 52.
Atlas(ours) 18.92 21.01 39.1 69.7 50.2 53.5 67.5 37.1 40.7 64.3 52.
Atlas++ (ours) 19.04 20.03 39.7 69.7 51.1 53.2 68.2 37.4 40.9 64.4 53.
Atlas(MAG) 18.62 21.18 40.0 70.3 50.5 53.0 68.1 36.5 41.2 65.0 53.
Atlas(MAL) 19.07 21.46 38.8 69.2 50.5 53.6 67.3 36.1 41.0 64.5 52.
1.3B params / 100B tokens
Transformer++ 18.53 18.32 42.6 70.0 50.2 53.5 68.8 35.1 40.7 57.1 52.
DeepTransformers(ours) 15.67 12.63 49.4 72.6 57.0 58.8 71.1 37.5 41.6 61.5 56.
Dot(ours) 15.28 11.96 50.1 73.3 57.5 60.4 72.2 41.2 42.7 61.4 57.
RetNet 19.08 17.27 40.5 70.1 49.2 54.1 67.3 33.8 40.8 60.4 52.
Mamba2 16.56 12.56 45.7 71.9 55.7 55.2 72.5 37.9 40.2 60.1 54.
DeltaNet 17.71 16.88 42.5 70.7 50.9 53.3 68.5 35.7 40.2 55.3 52.
Gated DeltaNet 16.42 12.17 46.6 72.2 55.8 57.4 71.2 38.4 40.6 60.2 55.
Sambaâˆ— 16.13 13.29 44.9 70.9 53.4 55.6 68.8 36.2 40.0 62.1 54.
Gated DeltaNet-H2âˆ— 15.91 12.55 48.8 72.2 56.9 57.8 71.4 39.1 41.2 61.6 56.
Titans (LMM) 15.60 11.41 49.1 73.1 56.3 59.8 72.4 40.8 42.1 61.0 56.
Memora 15.90 12.04 48.7 73.1 56.0 57.4 71.5 37.9 40.2 61.3 55.
OmegaNet(ours) 14.91 11.26 49.7 73.4 57.6 59.7 72.6 40.3 42.4 62.1 57.
Atlas(ours) 14.97 10.98 50.1 73.9 57.3 60.2 72.8 41.0 42.9 62.8 57.
Atlas++ (ours) 14.40 10.72 50.8 73.5 59.4 61.1 71.3 43.7 42.5 61.9 58.
```
### 6.2 Long Context: Needle In a Haystack

One of our main motivations to designAtlasis to enhance the performance of long-term neural memory module in
long context tasks. Accordingly, to evaluate the effectiveness of our designs for improving the effective context length
and memory capacity, we perform an experiment on needle-in-haystack tasks of RULER (Hsieh et al. 2024) benchmark.
The performance ofAtlasand its hybrid variants, as well as our Transformer-like architectures and baselines are
reported in Table 3.Atlasshows very good performance compared to the recurrent baselines, outperforming modern
recurrent neural networks such as Titans and DeltaNet. Its hybrid variants further improve its effective context length,
effectively extrapolating to sequences withÃ— 4 of their training context size. We attribute this performance to the proposed
enhancements for the capacity of the memory. We further perform ablation studies to validate this claim. Also, our
Transformer-like architectures outperforms the baselines, even our hybrid variants ofAtlasin longer contexts. This
shows the importance of exponential feature mapping in longer sequences.

### 6.3 Long Context: BABILong Benchmark

To compare the effectiveness ofAtlaswith Titans (Behrouz, Zhong, et al. 2024) in ultra-large sequences, we further
evaluateAtlasâ€™s performance on BABILong benchmark (Kuratov et al. 2024). In this experiment, we follow Behrouz,


Table 3: Performance ofAtlasand baselines on S-NIAH task from RULER benchmark. The best results among simple

andhybrid models are highlighted.

```
Model
S-NIAH-PK S-NIAH-N S-NIAH-W
2K 4K 8K 16K 2K 4K 8K 16K 2K 4K 8K
TTT 98.4 98.8 98.0 88.4 60.2 36.6 10.2 4.4 78.8 28.0 4.
DeltaNet 96.8 98.8 98.6 71.4 47.2 15.4 12.8 5.4 46.2 20.0 1.
Titans (LMM) 99.8 98.4 98.2 96.2 100.0 99.8 93.4 80.2 90.4 89.4 85.
Atlas 100 99.2 98.0 97.0 100.0 100.0 93.0 84.0 93.2 90.6 86.
Samba 98.8 98.0 97.4 97.2 98.8 98.6 96.2 95.6 96.8 90.0 84.
Gated DeltaNet-H2âˆ— 99.2 97.8 97.4 98.4 98.0 97.8 96.2 95.8 97.4 96.8 88.
Atlas(MAG) 100 100 99.4 98.6 100 99.2 97.4 97.0 99.4 98.2 92.
Atlas(MAL) 99.8 99.6 98.4 96.8 99.8 98.0 97.2 96.8 98.0 98.4 92.
DeepTransformers 100 100 98.2 97.8 100 98.8 97.8 94.0 95.8 92.2 88.
Dot 100 100 99.6 98.6 100 100 97.8 96.8 99.0 98.4 93.
```
Zhong, et al. (2024) and use MAC architecture but without persistent memory tokens. We also follow the original setup in
the benchmark and fine-tune our model. The results are reported in Figure 4. WhileAtlasshows competitive and on par
performance with Titans until 1M context length, the performance of Titans drops in 10M.Atlas, however, maintains its
performance and achieve +80% accuracy in 10M context length. We attribute this to more powerful memory; in terms of
(1) memory management (i.e., the use of Muon), (2) better memory capacity due to polynomial kernels, and (3) its nature to
memorize the context, instead of individual tokens.

In previous sections, we show the effectiveness of our Transformer-like architectures (i.e.,DeepTransformers andDot) in
both language modeling and long-context needle-in-haystack tasks. From now on, we focus on our recurrent architectures
(i.e.,Atlas, andOmegaNet) to show the importance of presented improvements.

### 6.4 Learnability Experiments

We have also performed some small-scale experiments to analyze the function-learning capability of small MLPs in an
online fashion. In this setting, we have a sequence of tuples(ğ‘– 1 ,ğ‘œ 1 ),.. .(ğ‘–ğ‘¡,ğ‘œğ‘¡)with bothğ‘–ğ‘—,ğ‘œğ‘—âˆˆRğ‘‘for allğ‘—. We train an
MLPMin an online fashion to minimizelossğ‘—=âˆ¥ğ‘–ğ‘—âˆ’ğ‘œğ‘—âˆ¥^22 /âˆ¥ğ‘œğ‘—âˆ¥^22 â€“ specifically, we compute the gradient at time stepğ‘—as
âˆ‡M.paramslossğ‘—and use standard optimizers such as Adam, Rmsprop and SGD to update the parameters. Such experiments
help us understand the representation power of the models we use to represent memory and the power of optimization
algorithms to quickly learn the underlying sequence mapping.

We study five different sequence to sequence functions:

```
1.Low Rank Mappings: We sample a random low rank matrixW=XYwithXâˆˆRğ‘‘Ã—ğ‘˜andYâˆˆRğ‘˜Ã—ğ‘‘. We then
sampleğ‘– 1 ,.. .,ğ‘–ğ‘¡randomly from a Gaussian distribution and setğ‘œğ‘—=WTÂ·ğ‘–ğ‘—for allğ‘—âˆˆ [ğ‘¡].
2.MLP Mappings: We sample an MLPMwith 1 input, 1 hidden and 1 output layer which uses GELU non-linearity.
We set the hidden dimension toğ‘‘so that there is no expansion. We then sampleğ‘– 1 ,.. .,ğ‘–ğ‘¡randomly from a Gaussian
distribution and then setğ‘œğ‘—=M(ğ‘–ğ‘—)for allğ‘—âˆˆ [ğ‘¡].
3.Attention+MLP Mapping: We sample(ğ‘– 1 ,.. .,ğ‘–ğ‘¡)from a Gaussian distribution and an MLPMas above. We
additionally sample threeğ‘‘Ã—ğ‘‘matricesWQ,WKandWVand computeğ‘ğ‘—=WQTÂ·ğ‘–ğ‘—,ğ‘˜ğ‘—=WKTÂ·ğ‘–ğ‘—andğ‘£ğ‘—=WKTÂ·ğ‘–ğ‘—
for allğ‘— âˆˆ [ğ‘¡]. We then computeğ‘œ 1 â€²,.. .,ğ‘œğ‘¡â€²as outputs of the causal masked attention mechanism applied on
{ğ‘ğ‘—}ğ‘—âˆˆ[ğ‘¡],{ğ‘˜ğ‘—}ğ‘—âˆˆ[ğ‘¡],{ğ‘£ğ‘—}ğ‘—âˆˆ[ğ‘¡]and finally computeğ‘œğ‘—=M(ğ‘œğ‘—).
4.Attention Outputs as Inputs: We do the same as above except that we outputğ‘œâ€²ğ‘—as the input sequence andğ‘œğ‘—as
the output sequence.
5.Sliding Window Attention + MLP Mapping: We do the same as inAttention + MLP Mappingsetting except
that we use a sliding window attention instead of full attention. We use a sliding window of 512 in our experiments.
```

(a)Mwith 2 hidden layers and no expansion. (b)Mwith 3 hidden layers and no expansion.

(c)Mwith 2 hidden layers and 4x expansion. (d)Mwith 3 hidden layers and 4x expansion.

```
Figure 6: Loss curves for different setting with various hyperparameters
```

Table 4: Performance ofAtlas,OmegaNet, and baselines on the synthetic benchmark of MAD (Poli et al. 2024).Atlas
outperforms all the baselines, including Transformers.

```
Compression (Noisy) ICR Fuzzy ICR Selective Memorization Average
Copying
Transformers 49.4 100 48.2 95.9 83.8 75.
Gated DeltaNet 44.8 100 32.5 96.2 81.7 71.
Titans 49.6 100 49.7 99.4 83.5 76.
OmegaNet(ours) 50.9 100 54.2 99.6 90.2 78.
Atlas(ours) 51.6 100 54.9 99.6 91.4 79.
```
Note that the settings 3 and 5 are much harder to learn since they require (partially) memorizing the previous inputs and
outputs to be able to learn the function that mapsğ‘–ğ‘—toğ‘œğ‘—, whereas the settings 1, 2 and 4 do not need to memorize the
previous input-output pairs and just need to learn the underlying low-rank matrix or the MLP that maps the inputs to
outputs.

The setting 4 is slightly different to setting 2 in that the inputs are not-independent at each time step and are correlated
by the attention mechanism we use to compute the inputs. Thus a strong learning algorithm maybe able to utilize the
underlying correlations to learn the mapping faster in setting 4 versus setting 2.

Table 5: The performance of our models (Atlas, andOmegaNet) com-
pared to baselines. While still Transformers achieve the best results in
in-context recall tasks, our design of context memorization and polyno-
mial feature maps can close the gap with Transformers.

```
SWDE NQ DROP FDA SQUAD TQA Average
Transformers 84.9 23.0 28.4 72.5 48.1 64.4 53.
Gated DeltaNet 63.2 19.1 26.7 33.4 39.6 59.7 40.
Titans 65.1 20.7 27.2 37.3 42.6 61.0 42.
OmegaNet(ours) 67.4 21.1 27.2 39.0 43.2 60.9 43.
Atlas(ours) 66.8 21.9 27.4 40.7 44.1 61.3 43.
```
```
Table 6: Ablation Study onAtlas. All compo-
nents ofAtlasare positively contributing to its
performance.
```
```
Model Language Modelingpplâ†“ C.S. Reasoningaccâ†‘
Atlas 19.97 52.
+Gated MLP Memory 19.53 53.
+Attn(MAG) 19.90 53.
+Attn(MAL) 20.26 52.
Linear Memory 21.03 49.
w/o Muon 19.65 52.
ğ‘= 1 21.98 49.
w/o Polynomial Mapping 22.14 50.
```
We setğ‘‘= 256 and show the loss curves vs sequence position for all the five settings with function learning MLPMbeing
defined and trained with different settings in Figure 6. We can see that in all the settings, the model learns non-trivial
mappings from inputs to outputs with theğ‘™ğ‘œğ‘ ğ‘ ğ‘—=âˆ¥ğ‘–ğ‘—âˆ’ğ‘œğ‘—âˆ¥^22 /âˆ¥ğ‘œğ‘—âˆ¥^22 being smaller than 1 eventually. Most notably, the
correlations in inputs in setting 4 induced by the attention mechanism makes the model quickly learn the mapping
compared to in setting 2 and the models usually learn the best in setting 1 which is the least complex function.

The models do the worst in settings 3 and 5 which require the models to (partially) memorize the inputs and outputs to
learn the attention mechanism outputs. Surprisingly, the models learn to do better in setting 3 vs setting 5, when we would
expect that capacity requirement for setting 3 to be higher than setting 5. We hypothesize that the learning algorithm is
unable to make the model â€˜forgetâ€™ old inputs which makes the loss worse in sliding window setting when compared to
global attention setting. A caveat of our analysis is that, the attention computation is done on randomly initialized vectors
and hence the attention matrix is usually not spiky, unlike in the attention matrix for trained set of query, key and value
vectors in LLMs. This leads to attention outputs being close to the mean of value vectors in the context.

### 6.5 Additional Experiments: In-context Recall, MAD Synthetic Benchmark, and Associative

### Recall

In this section, we first evaluate the performance of our models on MAD benchmark, a synthetic benchmark that evaluate
the performance of models in recall, memorization, compression, and copying tasks (Poli et al. 2024). The results are
reported in Table 4.Atlasachieves the best results in all aspects, particularly in memorization, which shows the importance
of its components for enhancing the memory capacity.


In-context recall tasks is one of the most challenging benchmarks for recurrent neural networks. In this section, we follow
Arora, Eyuboglu, Zhang, et al. (2024) and perform experiments on SWDE (Lockard et al. 2019), NQ (Kwiatkowski et al. 2019),
DROP (Dua et al. 2019), FDA (Arora, Yang, et al. 2023), SQUAD (Rajpurkar et al. 2016), and TQA (Kembhavi et al. 2017) to
evaluate and compare the performance ofAtlaswith baselines and Transformers. The results are reported in Table 5.
While Transformers still achieve the best results in in-context recall tasks,AtlasandOmegaNetshows competitive
performance and performs better than state-of-the-art recurrent models. We again attribute this performance to better
memory management and capacity.

Figure 7: The results for associative mem-
ory recall.

```
Figure 8: Scaling patterns ofAtlas, andOmegaNetwith respect to (Left) training
context length, and (Right) FLOPs.
```
Finally, following Yang, Wang, Zhang, et al. (2024) and Arora, Eyuboglu, Timalsina, et al. (2023) we evaluate the performance
ofAtlasandDotin Multi-Query Associative Recall (MQAR) task (Arora, Eyuboglu, Timalsina, et al. 2023). The results are
reported in Figure 7. Both models show good performance compared to baselines andAtlasachieve the best performance
per memory size compared to state-of-the-art models such as DeltaNet (Yang, Wang, Zhang, et al. 2024).

### 6.6 Ablation Study and Scaling Patterns

In this section, we perform an ablation study on the differernt components ofAtlas, and also evaluate its scaling patterns
with respect to the number of parameters and also the context length of the training. The results for ablation study
are reported in Table 6. The results show that: (1) more powerful memory architectures such as gated MLP can further
enhance the performance ofAtlas; (2) The hybrid variants further improve the performance, where MAG shows better
improvement compared to MAL architecture; (3) Polynomial mappings as well as deep memory are particularly important
when we use context memorization (i.e., Omega rule). Figure 5 also shows the effect of local context length (i.e.,ğ‘) on the
performance of the model. With the increase ofğ‘we can achieve better performance, mainly due to the gating parameters
ofğ›¾that can prune the context, whenever it is needed.

Model Size.Figure 8 shows the scaling pattern ofAtlas, andOmegaNet, with respect to number of parameters and
compared to baseline. Both models achieve a good scaling pattern with increasing the model size, achieving lower perplexity
in all scales compared to baselines.

Context Length.Figure 8 shows the scaling pattern ofAtlas, andOmegaNet, with respect to the context length and
compared to baseline. Both models due to high memory capacity can scale well, when increasing the context length.

## 7 Conclusion

We introducedAtlas, a new long-term memory module designed to address the core limitations of modern recurrent
models in long-context understanding: limited memory capacity, online-only updates, and weak memory management. Our
proposed sliding window learning rule, higher-order feature mappings, and advanced memory optimizers offer a principled
and scalable approach to overcoming these challenges. Empirically, our modelsâ€”OmegaNet,Atlas,DeepTransformers,
andDotâ€”achieve consistent improvements over Transformers and recent RNN variants across diverse benchmarks.
Theoretically, we provided insight into memory capacity and optimization dynamics, offering explanations for the context
length limitations observed in prior works.


## References

[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,
Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. â€œGpt-4 technical reportâ€. In:arXiv preprint arXiv:2303.
(2023).
[2] Zeyuan Allen-Zhu. â€œPhysics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layersâ€. In:
SSRN Electronic Journal(May 2025).https://ssrn.com/abstract=5240330.
[3] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher
RÃ©. â€œZoology: Measuring and improving recall in efficient language modelsâ€. In:arXiv preprint arXiv:2312.
(2023).
[4] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, James Zou, Atri Rudra, and Christopher
Re. â€œSimple linear attention language models balance the recall-throughput tradeoffâ€. In:Forty-first International
Conference on Machine Learning. 2024.url:https://openreview.net/forum?id=e93ffDcpH3.
[5] Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, and Christopher
RÃ©. â€œLanguage models enable simple systems for generating structured views of heterogeneous data lakesâ€. In:arXiv
preprint arXiv:2304.09433(2023).
[6] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. â€œNeural machine translation by jointly learning to align
and translateâ€. In:arXiv preprint arXiv:1409.0473(2014).
[7] Eric B Baum. â€œOn the capabilities of multilayer perceptronsâ€. In:Journal of Complexity4.3 (1988), pp. 193â€“215.issn:
0885-064X.doi:https://doi.org/10.1016/0885-064X(88)90020-9.url:https://www.sciencedirect.com/
science/article/pii/0885064X88900209.
[8] Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni. â€œItâ€™s All Connected: A Journey Through
Test-Time Memorization, Attentional Bias, Retention, and Online Optimizationâ€. In:arXiv preprint arXiv:2504.
(2025).
[9] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. â€œTitans: Learning to memorize at test timeâ€. In:arXiv preprint
arXiv:2501.00663(2024).
[10] Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. â€œLow-rank bottleneck in
multi-head attention modelsâ€. In:International conference on machine learning. PMLR. 2020, pp. 864â€“873.
[11] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. â€œBirth of a transformer: A
memory viewpointâ€. In:Advances in Neural Information Processing Systems36 (2023), pp. 1560â€“1588.
[12] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. â€œPiqa: Reasoning about physical commonsense in
natural languageâ€. In:Proceedings of the AAAI conference on artificial intelligence. Vol. 34. 2020, pp. 7432â€“7439.
[13] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
â€œBoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questionsâ€. In:Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers). Ed. by Jill Burstein, Christy Doran, and Thamar Solorio. Minneapolis, Minnesota:
Association for Computational Linguistics, June 2019, pp. 2924â€“2936.doi:10.18653/v1/N19-1300.url:https:
//aclanthology.org/N19-1300/.
[14] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Taf jord.
â€œThink you have solved question answering? try arc, the ai2 reasoning challengeâ€. In:arXiv preprint arXiv:1803.
(2018).
[15] Thomas M. Cover. â€œGeometrical and Statistical Properties of Systems of Linear Inequalities with Applications in
Pattern Recognitionâ€. In:IEEE Transactions on Electronic ComputersEC-14.3 (1965), pp. 326â€“334.doi:10.1109/PGEC.
1965.264137.
[16] RÃ³bert CsordÃ¡s, Christopher Potts, Christopher D Manning, and Atticus Geiger. â€œRecurrent Neural Networks Learn to
Store and Generate Sequences using Non-Linear Representationsâ€. In:Proceedings of the 7th BlackboxNLP Workshop:
Analyzing and Interpreting Neural Networks for NLP. 2024, pp. 248â€“262.
[17] Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung,
Jan Kautz, Carlos Guestrin, et al. â€œOne-Minute Video Generation with Test-Time Trainingâ€. In:arXiv preprint
arXiv:2504.05298(2025).
[18] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. â€œDROP: A reading
comprehension benchmark requiring discrete reasoning over paragraphsâ€. In:arXiv preprint arXiv:1903.00161(2019).
[19] Jianqing Fan.Local polynomial modelling and its applications: monographs on statistics and applied probability 66.
Routledge, 2018.


[20] Xavier Gonzalez, Andrew Warrington, Jimmy Smith, and Scott Linderman. â€œTowards scalable and stable paralleliza-
tion of nonlinear rnnsâ€. In:Advances in Neural Information Processing Systems37 (2024), pp. 5817â€“5849.
[21] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. â€œLiquid
Structural State-Space Modelsâ€. In:The Eleventh International Conference on Learning Representations. 2023.url:
https://openreview.net/forum?id=g4OTKRKfS7R.
[22] Zexue He, Leonid Karlinsky, Donghyun Kim, Julian McAuley, Dmitry Krotov, and Rogerio Feris. â€œCAMELoT: Towards
Large Language Models with Training-Free Consolidated Associative Memoryâ€. In:arXiv preprint arXiv:2402.13449
(2024).
[23] Donald Olding Hebb.The organization of behavior: A neuropsychological theory. Psychology press, 2005.
[24] Dan Hendrycks and Kevin Gimpel. â€œGaussian error linear units (gelus)â€. In:arXiv preprint arXiv:1606.08415(2016).
[25] John J Hopfield. â€œNeural networks and physical systems with emergent collective computational abilities.â€ In:
Proceedings of the national academy of sciences79.8 (1982), pp. 2554â€“2558.
[26] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg.
â€œRULER: Whatâ€™s the Real Context Size of Your Long-Context Language Models?â€ In:First Conference on Language
Modeling. 2024.url:https://openreview.net/forum?id=kIoBbc76Sy.
[27] Jerry Yao-Chieh Hu, Dennis Wu, and Han Liu. â€œProvably optimal memory capacity for modern hopfield models:
Transformer-compatible dense associative memories as spherical codesâ€. In:arXiv preprint arXiv:2410.23126(2024).
[28] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. â€œTransformer quality in linear timeâ€. In:International conference
on machine learning. PMLR. 2022, pp. 9099â€“9117.
[29] Guang-Bin Huang. â€œLearning capability and storage capacity of two-hidden-layer feedforward networksâ€. In:IEEE
Transactions on Neural Networks14.2 (2003), pp. 274â€“281.doi:10.1109/TNN.2003.809401.
[30] Kazuki Irie, Imanol Schlag, Robert Csordas, and Jurgen Schmidhuber. â€œGoing beyond linear transformers with
recurrent fast weight programmersâ€. In:Advances in neural information processing systems34 (2021), pp. 7703â€“7717.
[31] Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein.Muon:
An optimizer for hidden layers in neural networks. 2024.url:https://kellerjordan.github.io/posts/muon/.
[32] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. â€œPolySketchFormer: Fast Transformers via Sketching Polyno-
mial Kernelsâ€. In:Forty-first International Conference on Machine Learning. 2024.url:https://openreview.net/
forum?id=ghYrfdJfjK.
[33] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. â€œPolySketchFormer: Fast Transformers via Sketching Polyno-
mial Kernelsâ€. In:Proceedings of the 41st International Conference on Machine Learning. Ed. by Ruslan Salakhutdinov,
Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp. Vol. 235.
Proceedings of Machine Learning Research. PMLR, July 2024, pp. 22748â€“22770.url:https://proceedings.mlr.
press/v235/kacham24a.html.
[34] Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova,
Alexandre RamÃ©, Morgane RiviÃ¨re, et al. â€œGemma 3 technical reportâ€. In:arXiv preprint arXiv:2503.19786(2025).
[35] M. Karami and V. Mirrokni.Lattice: Learning to Efficiently Compress the Memory. 2025.
[36] Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and
Noah A. Smith. â€œFinetuning Pretrained Transformers into RNNsâ€. In:Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing. Ed. by Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott
Wen-tau Yih. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021,
pp. 10630â€“10643.doi:10.18653/v1/2021.emnlp-main.830.url:https://aclanthology.org/2021.emnlp-
main.830/.
[37] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. â€œTransformers are rnns: Fast au-
toregressive transformers with linear attentionâ€. In:International conference on machine learning. PMLR. 2020,
pp. 5156â€“5165.
[38] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. â€œAre you
smarter than a sixth grader? textbook question answering for multimodal machine comprehensionâ€. In:Proceedings
of the IEEE Conference on Computer Vision and Pattern recognition. 2017, pp. 4999â€“5007.
[39] Dmitry Krotov. â€œHierarchical associative memoryâ€. In:arXiv preprint arXiv:2107.06446(2021).
[40] Dmitry Krotov and John J Hopfield. â€œDense associative memory for pattern recognitionâ€. In:Advances in neural
information processing systems29 (2016).
[41] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Igorevich Sorokin, Artyom Sorokin, and Mikhail
Burtsev. â€œBABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystackâ€. In:The Thirty-


eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024.url:https :
//openreview.net/forum?id=u7m2CG84BQ.
[42] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle
Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. â€œNatural questions: a benchmark for question answering
researchâ€. In:Transactions of the Association for Computational Linguistics7 (2019), pp. 453â€“466.
[43] Chengxuan Li, Di Huang, Zeyu Lu, Yang Xiao, Qingqi Pei, and Lei Bai. â€œA survey on long video generation: Challenges,
methods, and prospectsâ€. In:arXiv preprint arXiv:2403.16407(2024).
[44] Xiaoyu Li, Yuanpeng Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. â€œOn the expressive power of modern hopfield
networksâ€. In:arXiv preprint arXiv:2412.05562(2024).
[45] Yi Heng Lim, Qi Zhu, Joshua Selfridge, and Muhammad Firmansyah Kasim. â€œParallelizing non-linear sequential
models over the sequence lengthâ€. In:The Twelfth International Conference on Learning Representations. 2024.url:
https://openreview.net/forum?id=E34AlVLN0v.
[46] Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, and Qiang Liu. â€œLonghorn: State space models are amortized
online learnersâ€. In:arXiv preprint arXiv:2407.14207(2024).
[47] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. â€œLost in
the middle: How language models use long contextsâ€. In:Transactions of the Association for Computational Linguistics
12 (2024), pp. 157â€“173.
[48] Colin Lockard, Prashant Shiralkar, and Xin Luna Dong. â€œOpenceres: When open information extraction meets the
semi-structured webâ€. In:Proceedings of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019, pp. 3047â€“3056.
[49] Carlo Lucibello and Marc MÃ©zard. â€œExponential capacity of dense associative memoriesâ€. In:Physical Review Letters
132.7 (2024), p. 077301.
[50] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. â€œPointer Sentinel Mixture Modelsâ€. In:
International Conference on Learning Representations. 2017.url:https://openreview.net/forum?id=Byj72udxe.
[51] William Merrill, Jackson Petty, and Ashish Sabharwal. â€œThe Illusion of State in State-Space Modelsâ€. In:Forty-first
International Conference on Machine Learning. 2024.url:https://openreview.net/forum?id=QZgo9JZpLq.
[52] Guido Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. â€œOn the number of linear regions of deep
neural networksâ€. In:Proceedings of the 28th International Conference on Neural Information Processing Systems -
Volume 2. NIPSâ€™14. Montreal, Canada: MIT Press, 2014, pp. 2924â€“2932.
[53] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. â€œMetalearned neural memoryâ€. In:
Advances in Neural Information Processing Systems32 (2019).
[54] Tsendsuren Munkhdalai and Hong Yu. â€œNeural semantic encodersâ€. In:Proceedings of the conference. Association for
Computational Linguistics. Meeting. Vol. 1. NIH Public Access. 2017, p. 397.
[55] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle,
Marco Baroni, Gemma Boleda, and Raquel Fernandez. â€œThe LAMBADA dataset: Word prediction requiring a broad
discourse contextâ€. In:Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers). Ed. by Katrin Erk and Noah A. Smith. Berlin, Germany: Association for Computational Linguistics,
Aug. 2016, pp. 1525â€“1534.doi:10.18653/v1/P16-1144.url:https://aclanthology.org/P16-1144/.
[56] Razvan Pascanu, Guido Montufar, and Yoshua Bengio.On the number of response regions of deep feed forward networks
with piece-wise linear activations. 2014. arXiv:1312.6098 [cs.LG].url:https://arxiv.org/abs/1312.6098.
[57] Guilherme Penedo, Hynek Kydlicek, Anton Lozhkov, Margaret Mitchell, Colin A Raffel, Leandro Von Werra, Thomas
Wolf, et al. â€œThe fineweb datasets: Decanting the web for the finest text data at scaleâ€. In:Advances in Neural
Information Processing Systems37 (2024), pp. 30811â€“30849.
[58] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao,
Xin Cheng, Michael Nguyen Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He,
Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, BartÅ‚omiej Koptyra, Hayden Lau, Jiaju Lin, Krishna
Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, StanisÅ‚aw Wozniak,
Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. â€œRWKV: Reinventing RNNs for the Transformer Eraâ€.
In:The 2023 Conference on Empirical Methods in Natural Language Processing. 2023.url:https://openreview.net/
forum?id=7SaXczaBpG.
[59] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian
Du, Teddy Ferdinan, Haowen Hou, et al. â€œEagle and finch: Rwkv with matrix-valued states and dynamic recurrenceâ€.
In:arXiv preprint arXiv:2404.05892(2024).


[60] Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Haowen Hou, Janna Lu, William Merrill, Guangyu Song,
Kaifeng Tan, Saiteja Utpala, et al. â€œRwkv-7" goose" with expressive dynamic state evolutionâ€. In:arXiv preprint
arXiv:2503.14456(2025).
[61] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bjorn Deiseroth, Kristian Kersting, Taiji Suzuki,
Brian Hie, Stefano Ermon, Christopher Re, et al. â€œMechanistic design and scaling of hybrid architecturesâ€. In:arXiv
preprint arXiv:2403.17844(2024).
[62] DL Prados and SC Kak. â€œNeural network capacity using delta ruleâ€. In:Electronics Letters25.3 (1989), pp. 197â€“199.
[63] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. â€œSquad: 100,000+ questions for machine
comprehension of textâ€. In:arXiv preprint arXiv:1606.05250(2016).
[64] Hubert Ramsauer, Bernhard SchÃ¤fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleit-
ner, Thomas Adler, David Kreil, Michael K Kopp, GÃ¼nter Klambauer, Johannes Brandstetter, and Sepp Hochreiter.
â€œHopfield Networks is All You Needâ€. In:International Conference on Learning Representations. 2021.url:https:
//openreview.net/forum?id=tL89RnzIiCd.
[65] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. â€œSamba: Simple Hybrid State Space
Models for Efficient Unlimited Context Language Modelingâ€. In:arXiv preprint arXiv:2406.07522(2024).
[66] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. â€œWinogrande: An adversarial winograd
schema challenge at scaleâ€. In:Communications of the ACM64.9 (2021), pp. 99â€“106.
[67] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. â€œSocial IQa: Commonsense Reasoning
about Social Interactionsâ€. In:Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Ed. by Kentaro Inui,
Jing Jiang, Vincent Ng, and Xiaojun Wan. Hong Kong, China: Association for Computational Linguistics, Nov. 2019,
pp. 4463â€“4473.doi:10.18653/v1/D19-1454.url:https://aclanthology.org/D19-1454/.
[68] Siddhartha Satpathi and Rayadurgam Srikant. â€œThe dynamics of gradient descent for overparametrized neural
networksâ€. In:Learning for Dynamics and Control. PMLR. 2021, pp. 373â€“384.
[69] Imanol Schlag, Kazuki Irie, and JÃ¼rgen Schmidhuber. â€œLinear transformers are secretly fast weight programmersâ€. In:
International Conference on Machine Learning. PMLR. 2021, pp. 9355â€“9366.
[70] JH Schmidhuber. â€œLearning to control fast-weight memories: An alternative to recurrent nets. Accepted for publication
inâ€. In:Neural Computation(1992).
[71] JÃ¼rgen Schmidhuber. â€œReducing the ratio between learning complexity and number of time varying variables in fully
recurrent netsâ€. In:ICANNâ€™93: Proceedings of the International Conference on Artificial Neural Networks Amsterdam,
The Netherlands 13â€“16 September 1993 3. Springer. 1993, pp. 460â€“463.
[72] JÃ¼rgen Schmidhuber and Sepp Hochreiter. â€œLong Short-term Memoryâ€. In:Neural Computation MIT-Press(1997).
[73] Mark SchÃ¶ne, Babak Rahmani, Heiner Kremer, Fabian Falck, Hitesh Ballani, and Jannes Gladrow. â€œImplicit Language
Models are RNNs: Balancing Parallelization and Expressivityâ€. In:arXiv preprint arXiv:2502.07827(2025).
[74] Jack Sherman and Winifred J Morrison. â€œAdjustment of an inverse matrix corresponding to a change in one element
of a given matrixâ€. In:The Annals of Mathematical Statistics21.1 (1950), pp. 124â€“127.
[75] Julien Siems, Timur Carstensen, Arber Zela, Frank Hutter, Massimiliano Pontil, and Riccardo Grazzi. â€œDeltaProduct:
Increasing the Expressivity of DeltaNet Through Products of Householdersâ€. In:arXiv preprint arXiv:2502.10297
(2025).
[76] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. â€œSimplified State Space Layers for Sequence Modelingâ€.
In:The Eleventh International Conference on Learning Representations. 2023.url:https://openreview.net/forum?
id=Ai8Hw3AXqks.
[77] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong
Wang, Sanmi Koyejo, et al. â€œLearning to (learn at test time): Rnns with expressive hidden statesâ€. In:arXiv preprint
arXiv:2407.04620(2024).
[78] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. â€œRetentive
network: A successor to transformer for large language modelsâ€. In:arXiv preprint arXiv:2307.08621(2023).
[79] W Scott Terry.Learning and memory: Basic principles, processes, and procedures. Routledge, 2017.
[80] Matteo Tiezzi, Michele Casoni, Alessandro Betti, Tommaso Guidi, Marco Gori, and Stefano Melacci. â€œOn the
resurgence of recurrent models for long sequences: Survey and research opportunities in the transformer eraâ€. In:
arXiv preprint arXiv:2402.08132(2024).
[81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia
Polosukhin. â€œAttention is All you Needâ€. In:Advances in Neural Information Processing Systems. Ed. by I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc., 2017.url:


https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-
Paper.pdf.
[82] Johannes Von Oswald, Maximilian Schlegel, Alexander Meulemans, Seijin Kobayashi, Eyvind Niklasson, Nicolas
Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, et al. â€œUncovering mesa-optimization
algorithms in transformersâ€. In:arXiv preprint arXiv:2309.05858(2023).
[83] Ke Alexander Wang, Jiaxin Shi, and Emily B Fox. â€œTest-time regression: a unifying framework for designing sequence
models with associative memoryâ€. In:arXiv preprint arXiv:2501.12352(2025).
[84] Kaiyue Wen, Xingyu Dang, and Kaifeng Lyu. â€œRnns are not transformers (yet): The key bottleneck on in-context
retrievalâ€. In:arXiv preprint arXiv:2402.18510(2024).
[85] Bernard Widrow and Marcian E Hoff.Adaptive switching circuits. 1988.
[86] David J Willshaw, O Peter Buneman, and Hugh Christopher Longuet-Higgins. â€œNon-holographic associative memoryâ€.
In:Nature222.5197 (1969), pp. 960â€“962.
[87] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. â€œGated Delta Networks: Improving Mamba2 with Delta Ruleâ€. In:
arXiv preprint arXiv:2412.06464(2024).
[88] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. â€œGated Linear Attention Transformers
with Hardware-Efficient Trainingâ€. In:Forty-first International Conference on Machine Learning. 2024.url:https:
//openreview.net/forum?id=ia5XvxFUJT.
[89] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. â€œParallelizing linear transformers with the delta
rule over sequence lengthâ€. In:Advances in Neural Information Processing Systems37 (2024), pp. 115491â€“115522.
[90] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. â€œHellaSwag: Can a Machine Really Finish
Your Sentence?â€ In:Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Ed. by
Anna Korhonen, David Traum, and Lluis Marquez. Florence, Italy: Association for Computational Linguistics, July
2019, pp. 4791â€“4800.doi:10.18653/v1/P19-1472.url:https://aclanthology.org/P19-1472/.
[91] Yufeng Zhang, Boyi Liu, Qi Cai, Lingxiao Wang, and Zhaoran Wang. â€œAn analysis of attention via the lens of
exchangeability and latent variable modelsâ€. In:arXiv preprint arXiv:2212.14852(2022).


## A Additional Related Work

Modern Linear Recurrent Neural Networks^2 .Recent research endeavors have concentrated on mitigating the quadratic
computational complexity and inherent limitations of Transformer models in processing long-context sequences. This
has led to the development of efficient recurrent alternatives, primarily motivated by their rapid inference and training
capabilities (Tiezzi et al. 2024). Initial advancements in this domain, exemplified by models such as RetNet (Sun, Dong,
et al. 2023), RWKV (Peng, Alcaide, et al. 2023), and S5 (Smith et al. 2023), employed data-independent transition matrices
coupled with Hebbian-like update mechanisms. Subsequently, a second generation of models emerged, incorporating
input-dependent parameters within these linear architectures (e.g., linear RNNs (Hasani et al. 2023; Smith et al. 2023),
RWKV6 (Peng, Goldstein, et al. 2024)). These models also explored more expressive memory updating rules, notably
those based on the delta rule (Liu, Wang, et al. 2024; Peng, Zhang, et al. 2025; Schlag et al. 2021; Yang, Kautz, et al. 2024;
Yang, Wang, Zhang, et al. 2024). Further evolution in this line of research has extended these memory architectures to
deeper models, while concurrently utilizing delta-rule-like update mechanisms (Sun, Li, et al. 2024) or data-dependent
momentum-based update rules with forget gating (Behrouz, Zhong, et al. 2024). More recently, to augment the performance
of delta-rule-based sequential models, Siems et al. (2025) have proposed the application of multiple gradient descent
updates per token, thereby yielding more expressive sequence models, particularly in state tracking tasks. In addition to the
above fast linear recurrent sequence models, several studies have focused on RNNs with non-linear recurrence (Behrouz,
Razaviyayn, et al. 2025; CsordÃ¡s et al. 2024; Gonzalez et al. 2024; Karami et al. 2025; Lim et al. 2024; Merrill et al. 2024;
SchÃ¶ne et al. 2025; Von Oswald et al. 2023), and how their training can be faster (Gonzalez et al. 2024; Lim et al. 2024;
SchÃ¶ne et al. 2025).

Fast Weight Programs.The conceptualization of linear layers as key-value associative memory systems can be traced
back to Hopfield networks (Hopfield 1982). This concept was subsequently developed in the context of fast weight
programmers, wherein dynamic fast programs are integrated into recurrent neural networks to serve as writable memory
stores (Schlag et al. 2021; Schmidhuber 1992; Schmidhuber 1993). Among the learning paradigms for such systems, Hebbian
learning (Hebb 2005) and the delta rule (Prados et al. 1989) have emerged as the most prominent. Both learning rules
have been the subject of extensive investigation within the existing literature (Irie et al. 2021; Munkhdalai, Sordoni, et al.
2019; Munkhdalai and Yu 2017; Schlag et al. 2021; Schmidhuber 1992; Yang, Kautz, et al. 2024; Yang, Wang, Zhang, et al.
2024).

Hopfield Networks.Our formulation is architecturally founded upon the broad concept of associative memory, wherein
the primary objective is to learn an underlying mapping between keys and values. Seminal work by Hopfield (1982) on
Hopfield Networks introduced one of the earliest neural architectures explicitly based on associative memory, defining
it through the minimization of an energy function for storing key-value pairs. Although traditional Hopfield networks
have seen diminished applicability in recent years, primarily due to constraints in vector-valued memory capacity and
the nature of their energy function, several contemporary studies have focused on enhancing their capacity through
various methodologies. These include efforts by Krotov (2021), Li, Li, et al. (2024), and Krotov and Hopfield (2016). Notably,
extensions to the energy function of these models, often incorporating exponential kernels, have been explored (Krotov
and Hopfield 2016; Lucibello et al. 2024). Furthermore, the relationship between these modernized Hopfield networks and
Transformer architectures has been a subject of recent investigation (Hu et al. 2024; Ramsauer et al. 2021).

## B MirasFramework

As discussed earlier, Behrouz, Razaviyayn, et al. (2025) formalized the concept of associative memory as:

Definition 2(Behrouz, Razaviyayn, et al. (2025)).Given a set of keysK âŠ†Rğ‘‘ğ‘˜and valuesV âŠ†Rğ‘‘ğ‘£, associative memory
is an mappingM:K â†’ V. Learning the associative memory is based on an objectiveL, calledAttentional Bias, that
determines the type of memory and its priorities:

```
Mâˆ—=arg min
M
```
##### L(M(K);V). (44)

(^2) Note that here the term â€œlinearâ€ refers to their fast training and inference procedures. This does not refer to their recurrence formula as some models
like Titans (Behrouz, Zhong, et al. 2024),Yaad, Moneta, Memora(Behrouz, Razaviyayn, et al. 2025), and TTT (Sun, Li, et al. 2024) are based onnon-linear
recurrence but fast at training and inference.


Optimizing this objective using an iterative algorithm (e.g., gradient descent) results in the memory update rule. Thus, the
sequence model is a meta in-context learner with two optimization levels:

```
1.Inner Loop: Where parameters of the memory module are optimized (i.e.,ğœ½M={ğ‘Š 1 ,ğ‘Š 2 ,.. .,ğ‘ŠLM,...}). In the inner
optimization loop, all other parameters from the model are considered hyperparameters and are fixed andnot
optimized.
2.Outer Loop: Where all other parameters of the model are optimized, such as linear projections, MLP layers,
convolutions, etc.
```
### B.1 Examples

As an example, one can define the linear attention as the optimization of dot-product similarity with gradient descent: i.e.,
â„“ Ìƒğ‘¡:=âŸ¨Mğ‘¡âˆ’ 1 kğ‘¡,vğ‘¡âŸ©. That is,

```
Mğ‘¡=Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡â„“ Ìƒğ‘¡(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡)=Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡âŸ¨Mğ‘¡âˆ’ 1 kğ‘¡,vğ‘¡âŸ© (45)
=Mğ‘¡âˆ’ 1 +ğœ‚ğ‘¡vğ‘¡kâŠ¤ğ‘¡. (46)
```
As an another example, if we use regression loss, instead of the dot-product similarity, we can obtain the DeltaNet (Schlag
et al. 2021):

```
Mğ‘¡=Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡âˆ¥Mğ‘¡kğ‘¡âˆ’vğ‘¡âˆ¥^22 =Iâˆ’ğœ‚ğ‘¡kğ‘¡kâŠ¤ğ‘¡Mğ‘¡âˆ’ 1 +vğ‘¡kâŠ¤ğ‘¡. (47)
```
## C Supporting Proofs

Proposition 1(Capacity ofâ„“ 2 Attentional Bias).LetMbe a matrix-valued memory withğ‘‘ğ‘£Ã—ğ‘‘ğ‘˜parameters that optimizes
the internal objective ofâ„“(Mğ‘¡;kğ‘¡,vğ‘¡)=âˆ¥Mğ‘¡kğ‘¡âˆ’vğ‘¡âˆ¥^22 with gradient descent.Mcan store the mapping of at mostO(ğ‘‘ğ‘˜)
pairs of(kğ‘–,vğ‘–)with linearly independent keys.

Proof.Letğ¾=[k 1 Â·Â·Â·kğ‘š] âˆˆRğ‘‘ğ‘˜Ã—ğ‘šandğ‘‰=[v 1 Â·Â·Â·vğ‘š] âˆˆRğ‘‘ğ‘£Ã—ğ‘š. The optimization problem becomes minimizing the
Frobenius normâˆ¥Mğ¾âˆ’ğ‘‰âˆ¥^22. Exact memorization requires solving the linear systemMğ¾=ğ‘‰.

Vectorizing the expression yields the system(ğ¾âŠ¤âŠ—ğ¼ğ‘‘ğ‘£)vec(M)=vec(ğ‘‰), which hasğ‘šğ‘‘ğ‘£scalar equations inğ‘‘ğ‘˜ğ‘‘ğ‘£unknowns.
When the keys are linearly independent,rank(ğ¾)=ğ‘š, and hence the system matrix has full row rankğ‘šğ‘‘ğ‘£. Solvability
thus requiresğ‘šğ‘‘ğ‘£â‰¤ğ‘‘ğ‘˜ğ‘‘ğ‘£, or equivalentlyğ‘šâ‰¤ğ‘‘ğ‘˜. This matches classic results on the storage capacity of linear associative
memories such as the Willshaw model and Hopfield networks, where capacity is tied to the rank of the input embedding
(Hopfield 1982; Willshaw et al. 1969).

Whenğ‘šâ‰¤ğ‘‘ğ‘˜andğ¾has full column rank, one can construct an exact interpolating solution via the Mooreâ€“Penrose
pseudoinverse:Mâˆ—=ğ‘‰ğ¾âŠ¤. ThenMâˆ—ğ¾=ğ‘‰ğ¾âŠ¤ğ¾=ğ‘‰, achieving zero training error. Thus the upper bound is tight.

Moreover, full-batch gradient descent on this objective with step size 0 <ğœ‚< 2 /ğœ†max(ğ¾ğ¾âŠ¤)yields iteratesMğ‘¡+ 1 =
Mğ‘¡âˆ’ğœ‚(Mğ‘¡ğ¾âˆ’ğ‘‰)ğ¾âŠ¤, which converge to the minimum-norm interpolating solutionMâ€ =ğ‘‰ğ¾âŠ¤whenğ‘šâ‰¤ğ‘‘ğ‘˜. This is a
well-known implicit bias of gradient descent in overparameterized linear models (Satpathi et al. 2021).

Finally, the same rank-based constraint governs the capacity of linear or multi-head attention modules. In such architectures,
the output context matrix has rank at mostrank(ğ¾) â‰¤ğ‘‘ğ‘˜, which directly limits their expressivity. Recent analyses identify
this â€œlow-rank bottleneckâ€ as a capacity-limiting effect in Transformers (Bhojanapalli et al. 2020). â–¡

Theorem 1(Effect of Deep Memory). LetM(Â·)be an MLP withLMâ‰¥ 2 layers,ğ‘‘ğ‘˜input dimension, andğ‘‘â„hidden

dimension. Then,M(Â·)can store the mapping of at leastO(ğ‘‘ğ‘˜ğ‘‘ğ‘£)and at mostO

##### 

##### ğ‘‘ğ‘˜ğ‘‘ğ‘£

##### ÃLM

```
ğ‘–= 1 min{ğ‘‘
```
```
(ğ‘—)
â„ }ğ‘—â‰¥ğ‘–ğ‘‘
```
```
(ğ‘—+ 1 )
â„
```
##### 

```
pairs of
```
(kğ‘–,vğ‘–)with linearly independent keys.

Early theoretical works established that even simple network architectures can memorize a significant number of input-
output mappings, with capacity often related to the number of network parameters (e.g., weights and biases) and the input
dimensionality Baum (1988), Cover (1965), and Huang (2003). For instance, Baum (1988) demonstrated that

##### ğ‘

```
ğ‘‘
```
##### 

neurons
are sufficient for a single-hidden-layer network with threshold units to memorizeğ‘input-label pairs fromRğ‘‘.


Networks employing Rectified Linear Units (ReLUs), exhibit a piecewise affine behavior. The input space is partitioned
into numerous linear regions, and within each region, the network computes a distinct affine transformation Montufar
et al. (2014) and Pascanu et al. (2014). This structure is pivotal for analyzing their expressive power and storage capacity.
The precise relationship between depth, width, the number of linear regions, and the ultimate capacity to store specific
key-value associations, especially with constraints like linearly independent keys, remains an active area of research.

Proof.Letğ‘šdenote the number of(kğ‘–,vğ‘–)pairs memorized exactly byM, and assume the keys{kğ‘–}ğ‘šğ‘–= 1 âŠ‚Rğ‘‘ğ‘˜are linearly

independent. Letğ‘‘â„(^0 ):=ğ‘‘ğ‘˜,ğ‘‘â„(LM):=ğ‘‘ğ‘£, and for each layer 1 â‰¤â„“â‰¤ LM, defineğ‘Š(â„“)âˆˆRğ‘‘

(â„“)
â„Ã—ğ‘‘
(â„“âˆ’ 1 )
â„. Biases are omitted for
simplicity.

Sinceğœ(ğ‘¥)=max( 0 ,ğ‘¥)is piecewise linear, the composition of linear maps and ReLU activations yields a piecewise affine
function. For any fixed activation pattern (i.e., fixed sign of pre-activations), the MLP acts as:

```
M(Â·)=ğ´Â·+ğµ, whereğ´=ğ‘Š(LM)ğ·(LMâˆ’^1 )ğ‘Š(LMâˆ’^1 )Â·Â·Â·ğ·(^1 )ğ‘Š(^1 ),
```
and eachğ·(â„“)is a diagonal{ 0 , 1 }matrix selecting the active units. Therefore, when all keys fall into the same linear region
(which occurs generically after a small perturbation),Mis a single affine transformation.

LetK:=[k 1 Â·Â·Â·kğ‘š] âˆˆRğ‘‘ğ‘˜Ã—ğ‘šandV:=[v 1 Â·Â·Â·vğ‘š] âˆˆRğ‘‘ğ‘£Ã—ğ‘š. Exact memorization impliesğ´K=V, so:

```
rank(V) â‰¤rank(ğ´), ğ‘š=rank(K) â‰¤min{rank(ğ´),ğ‘‘ğ‘˜}.
```
Now observe:

```
ğ´=ğ‘Š(LM)ğ·(LMâˆ’^1 )ğ‘Š(LMâˆ’^1 )
| {z }
ğ‘…LMâˆ’ 1
```
##### Â·Â·Â·ğ·(^1 )ğ‘Š(^1 )

```
| {z }
ğ‘… 1
```
##### ,

and thus the rank ofğ´is bounded by the minimal width encountered along each path times the immediate input dimension:

```
rank(ğ´) â‰¤
```
##### âˆ‘ï¸LM

```
ğ‘–= 1
```
##### 

```
min
ğ‘—â‰¥ğ‘–
```
##### ğ‘‘â„(ğ‘—)

##### 

##### ğ‘‘â„(ğ‘–)=O

##### ğ‘‘ğ‘˜ğ‘‘ğ‘£

##### Lâˆ‘ï¸M

```
ğ‘–= 1
```
```
min
ğ‘—â‰¥ğ‘–
```
##### ğ‘‘â„(ğ‘—)ğ‘‘â„(ğ‘–+^1 )

##### !

##### .

Hence,

##### ğ‘šâ‰¤ O

##### ğ‘‘ğ‘˜ğ‘‘ğ‘£

##### âˆ‘ï¸LM

```
ğ‘–= 1
```
```
min
ğ‘—â‰¥ğ‘–
```
##### ğ‘‘â„(ğ‘—)ğ‘‘â„(ğ‘–+^1 )

##### !

##### â–¡

Proposition 2(Memory Capacity with Polynomial Mapping). Letğœ™ğ‘(Â·)be a polynomial mapping with degree at most
ğ‘, andMbe a matrix-valued memory that optimizes the internal objective ofâ„“(Mğ‘¡;ğœ™ğ‘(kğ‘¡),vğ‘¡)=âˆ¥Mğ‘¡ğœ™ğ‘(kğ‘¡)âˆ’vğ‘¡âˆ¥^22 with
gradient descent.Mcan store the mapping of at mostO

##### 

##### ğ‘‘ğ‘˜ğ‘

##### 

pairs of(kğ‘–,vğ‘–)with linearly independent keys, whereğ‘‘ğ‘˜is the
dimension of keyskğ‘–.

Proof.Let us begin by analyzing the dimension of the lifted feature space induced byğœ™ğ‘. A monomial inğ‘‘ğ‘˜variables of

total degree exactlyâ„“has the formkğ›¼=

##### Ãğ‘‘ğ‘˜

##### ğ‘—= 1 ğ‘˜

```
ğ›¼ğ‘—
ğ‘— , whereğ›¼âˆˆN
```
ğ‘‘ğ‘˜and|ğ›¼|:=Ãğ‘‘ğ‘˜
ğ‘—= 1 ğ›¼ğ‘—=â„“. The number of such monomials
is given by the classical stars-and-bars formula, which counts the number of integer solutions toğ›¼ 1 +Â·Â·Â·+ğ›¼ğ‘‘ğ‘˜=â„“, yielding

```

ğ‘‘ğ‘˜+â„“âˆ’ 1
â„“
```
##### 

##### .


```
Summing over all degreesâ„“= 0 toğ‘gives the total number of monomials (i.e., the output dimension ofğœ™ğ‘),
```
##### ğ·=

##### âˆ‘ï¸ğ‘

```
â„“= 0
```
##### 

##### ğ‘‘ğ‘˜+â„“âˆ’ 1

##### â„“

##### 

##### =

##### 

##### ğ‘‘ğ‘˜+ğ‘

##### ğ‘

##### 

##### ,

```
where the final identity follows from the hockey-stick identity in combinatorics.
To characterize the memorization capacity, we reformulate the loss in matrix notation. LetÎ¦:=[ğœ™ğ‘(k 1 ) Â·Â·Â·ğœ™ğ‘(kğ‘š)] âˆˆ
Rğ·Ã—ğ‘šandğ‘‰:=[v 1 Â·Â·Â·vğ‘š] âˆˆRğ‘‘ğ‘£Ã—ğ‘š. Then the objective becomes
ğ¿(M)=^12 âˆ¥MÎ¦âˆ’ğ‘‰âˆ¥^22.
```
```
Exact memorization corresponds to the existence of a matrixMsuch thatMÎ¦=ğ‘‰. This is a linear system in whichM
acts on the columns ofÎ¦, so the rank ofÎ¦necessarily limits the number of independent targetsvğ‘–that can be fitted exactly.
By the sub-multiplicativity of rank, for any matricesğ´andğµ, we have
rank(ğ´ğµ) â‰¤min{rank(ğ´),rank(ğµ)}.
Applying this toMÎ¦yields
rank(MÎ¦) â‰¤rank(Î¦) â‰¤ğ·.
```
Now consider a case where the targetsv 1 ,.. .,vğ‘šare linearly independent; for instance, takeğ‘‰=[ğ‘’ 1 ,.. .,ğ‘’ğ‘š], the first
ğ‘šstandard basis vectors. Thenrank(ğ‘‰)=ğ‘š. Ifğ‘š>ğ·, we necessarily haverank(MÎ¦)<rank(ğ‘‰)for every choice of
M, implying that the systemMÎ¦=ğ‘‰is unsolvable. Hence, the loss remains strictly positive, and exact memorization is
impossible.
This establishes that no method, regardless of optimization procedure, can memorize more thanğ·=

##### ğ‘‘ğ‘˜+ğ‘

```
ğ‘
```
##### 

```
independent
input-output pairs under a degree-â‰¤ğ‘polynomial lifting. Since
```
##### ğ‘‘ğ‘˜+ğ‘

```
ğ‘
```
##### 

```
=Î˜(ğ‘‘ğ‘˜ğ‘)for fixedğ‘, the result follows: the
memorization capacity is bounded above byO(ğ‘‘ğ‘ğ‘˜). â–¡
```
## D Detailed Formulations of All Architectures

```
In this section, for the sake of clarity, we discuss the details of all architectures that we discuss through the paper:
```
### D.1 Deep Linear Attention (DLA)

```
We design Deep Linear Attention (DLA)â€”linear attention module that uses a deep MLP as the memory (KV cache)â€”as one
of the baselines of this study. Given inputxâˆˆRğ‘Ã—ğ‘‘in, we project the input into matrices of keys, values and queries:
```
##### Q=

##### Â©

##### Â­

##### Â­

##### Â«

```
q 1
..
.
qğ‘
```
##### Âª

##### Â®

##### Â®

##### Â¬

```
=xWğ‘„, K=
```
##### Â©

##### Â­

##### Â­

##### Â«

```
k 1
..
.
kğ‘
```
##### Âª

##### Â®

##### Â®

##### Â¬

```
=xWğ¾, V=
```
##### Â©

##### Â­

##### Â­

##### Â«

```
v 1
..
.
vğ‘
```
##### Âª

##### Â®

##### Â®

##### Â¬

```
=xWğ‘‰, (48)
```
```
whereWğ‘„,Wğ¾,andWğ‘‰are learnable linear layers. We then define memory as a learning module that optimizes the
inner-dot product similarity using gradient descent: i.e.,
min
M
```
```
âŸ¨M(kğ‘¡),vğ‘¡âŸ©
| {z }
â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡)
```
##### . (49)

```
The above optimization using gradient descent results in the following recurrence (we also add weight decay with
input-dependent parameterğ›¼ğ‘¡):
```
```
Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’ğœ‚ğ‘¡âˆ‡â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡) (50)
```
```
which in the case of linear memory (i.e.,Mğ‘¡=ğ‘Šğ‘¡âˆˆRğ‘‘Ã—ğ‘‘) it becomes:
```
```
ğ‘Šğ‘¡=ğ›¼ğ‘¡ğ‘Šğ‘¡âˆ’ 1 +vğ‘¡kğ‘¡âŠ¤, (51)
which is the formulation of gated linear attention. We use the same training process as other models (see Section 3.3).
```

### D.2 Sliding Window Linear Attention (SWLA)

The design of SWLA is the same as the design of DLA, but with the use of sliding window objective. That is, given keys,
values, and queries:

##### Q=

##### Â©

##### Â­

##### Â­

##### Â«

```
q 1
..
.
qğ‘
```
##### Âª

##### Â®

##### Â®

##### Â¬

```
=xWğ‘„, K=
```
##### Â©

##### Â­

##### Â­

##### Â«

```
k 1
..
.
kğ‘
```
##### Âª

##### Â®

##### Â®

##### Â¬

```
=xWğ¾, V=
```
##### Â©

##### Â­

##### Â­

##### Â«

```
v 1
..
.
vğ‘
```
##### Âª

##### Â®

##### Â®

##### Â¬

```
=xWğ‘‰, (52)
```
we optimize the internal objective of:

```
min
M
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
âŸ¨Mğ‘¡âˆ’ 1 (kğ‘–),vğ‘–âŸ©
| {z }
â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡)
```
##### . (53)

The above formulation, results in:

```
Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’âˆ‡â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡)=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğœ‚ğ‘–(ğ‘¡)âˆ‡âŸ¨Mğ‘¡âˆ’ 1 (kğ‘–),vğ‘–âŸ©, (54)
```
which in the case of linear memory (i.e.,Mğ‘¡=ğ‘Šğ‘¡âˆˆRğ‘‘Ã—ğ‘‘) it becomes:

##### Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’

##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğœ‚ğ‘–(ğ‘¡)vğ‘–kâŠ¤ğ‘–. (55)
```
### D.3 OmegaNet

In the design ofOmegaNet, we use replace the dot-prodcut similarity objective withâ„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡)=

##### Ãğ‘¡

ğ‘–=ğ‘¡âˆ’ğ‘+ 1 âˆ¥Mğ‘¡âˆ’^1 (ğœ™(kğ‘–))âˆ’
vğ‘–âˆ¥^22 ,which results in the recurrence of:

```
Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’âˆ‡â„“(Mğ‘¡âˆ’ 1 ;kğ‘¡,vğ‘¡)=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 âˆ’
```
##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğœ‚ğ‘–(ğ‘¡)âˆ‡âˆ¥Mğ‘¡âˆ’ 1 (ğœ™(kğ‘–))âˆ’vğ‘–âˆ¥^22. (56)
```
In the above formulation,ğœ™(.)is the polynomial feature mapping function.

### D.4 Atlas

In theAtlas, we use the same internal objective asOmegaNetbut we optimize it using Muon optimizer (Jordan et al.
2024) with weight decay. That is,

```
Mğ‘¡=ğ›¼ğ‘¡Mğ‘¡âˆ’ 1 +Newton-schulz5(Sğ‘¡) (57)
```
##### Sğ‘¡=ğœƒğ‘¡Sğ‘¡âˆ’ 1 âˆ’

##### âˆ‘ï¸ğ‘¡

```
ğ‘–=ğ‘¡âˆ’ğ‘+ 1
```
```
ğœ‚ğ‘–(ğ‘¡)âˆ‡âˆ¥Mğ‘¡âˆ’ 1 (ğœ™(kğ‘–))âˆ’vğ‘–âˆ¥^22. (58)
```
## E Experimental Details

In our experimental setup we follow recent studies on linear recurrent models (Behrouz, Razaviyayn, et al. 2025; Behrouz,
Zhong, et al. 2024; Yang, Kautz, et al. 2024), we use Wikitext (Merity et al. 2017), LMB (Paperno et al. 2016), PIQA (Bisk
et al. 2020), HellaSwag (Zellers et al. 2019), WinoGrande (Sakaguchi et al. 2021), ARC-easy (ARC-e) and ARC-challenge
(ARC-c) (Clark, Cowhey, et al. 2018), SIQA (Sap et al. 2019), and BoolQ (Clark, Lee, et al. 2019). Also, the baselines results
are from Behrouz, Razaviyayn, et al. (2025) and Behrouz, Zhong, et al. (2024). In the training, we use T5 tokenizer with a
vocabulary size of 32K and use training length of 4K tokens (2K for SWA). We employ AdamW optimizer with learning
rate of 4 ğ‘’- 4 with cosine annealing schedule with batch size of 0.5M tokens, and weight decay of 0. 1. The architectural


```
Table 7: Architectural Details.
```
#### Model Block Dim Head Peak LR Token

#### 170M 12 768 16 3e-3 15B

#### 340M 24 1024 16 1.5e-3 15B

#### 760M 24 1536 16 1.25e-3 30B

#### 1.3B 18 2048 8 7e-4 100B

details are also reported in Table 7. The baseline results for 1.3B are from Yang, Kautz, et al. (2024) and for 760M are from
Behrouz, Razaviyayn, et al. (2025) and Behrouz, Zhong, et al. (2024).

For the memory architecture, unless state otherwise, we use an MLP with 2 layers with expansion factor of 4 and GELU
activation function (Hendrycks et al. 2016). We also use residual connections and layer norm at the end of each chunk:
M(ğ‘¥)=ğ‘¥+ğ‘Š 1 ğœ(ğ‘Š 2 ğ‘¥).

## F Additional Experimental Results

In this section, we provide additional experimental results to support the design of our models, understand the effect of
different components and also evaluate their performance in long context, in-context recall and MAD tasks.

### F.1 Language Modeling and Common-sense Reasoning (Small Scale)

In Section 6 we presented a subset of results on language modeling and common-sense reasoning tasks. In this section, we
further report the results for all scales of models. The results are in Table 8.

State-of-the-art Results.Looking at the performance ofAtlasandOmegaNet, both architectures perform favorably
compared to modern linear recurrent models and Transformers, achieving lower perplexity and better accuracy in
downstream tasks. Even the fully recurrent version of these models outperform hybrid models such as Samba (Ren et al.
2024) and Gated DeltaNet-H2 (Yang, Kautz, et al. 2024). Using the hybrid variants of MAG and MAL further improve the
performance ofAtlas, which shows the complementary role of recurrent long-term memory and attention.

The Effect of Design.Comparing the performance ofAtlas,OmegaNet, and baselines SWLA and DLA, we can see
the role ofâ„“ 2 regression loss as the attentional bias. Also, the better performance of SWLA compared to GLA and RetNet
indicates the importance of memorizing the context, instead of memorizing individual tokens.


Table 8: Performance ofAtlasand baselines on language modeling and common-sense reasoning tasks. The best results

are highlightedhighlighted.

```
Model Wiki. LMB. LMB. PIQA Hella. Wino. ARC-e ARC-c SIQA BoolQ Avg.
pplâ†“ pplâ†“ accâ†‘ accâ†‘ acc_nâ†‘ accâ†‘ accâ†‘ acc_nâ†‘ accâ†‘ accâ†‘ â†‘
340M params / 15B tokens
Transformer++ 31.52 41.08 30.76 62.98 34.76 50.53 45.21 24.05 36.81 58.24 42.92
RetNet 32.50 49.73 28.24 62.61 34.15 50.91 44.27 23.62 36.79 59.72 42.54
GLA 28.51 43.02 28.73 64.05 35.96 50.00 54.19 24.29 37.13 58.39 44.09
Mamba 30.83 40.21 29.94 63.79 35.88 49.82 49.24 24.56 35.41 60.07 43.59
DeltaNet 28.65 47.30 28.43 63.52 35.95 49.63 52.68 25.37 37.96 58.79 44.04
TTT 27.44 34.19 30.06 63.97 35.71 50.08 53.01 26.11 37.32 59.83 44.51
Gated DeltaNet 27.01 30.94 34.11 63.08 38.12 51.60 55.28 26.77 34.89 59.54 45.42
Moneta 26.19 29.31 35.70 63.99 39.23 52.04 55.96 27.15 37.29 60.22 46.44
Yaad 26.61 29.11 34.09 64.93 39.86 51.12 54.75 28.64 33.82 60.29 45.93
Memora 27.16 30.44 33.68 65.21 39.17 51.23 53.40 27.99 34.1 59.29 45.51
DLA (ours) 27.93 35.09 30.8 62.9 36.2 50.4 53.5 26.7 37.1 59.7 44.76
SWDT (ours) 26.98 33.95 32.4 63.1 38.2 50.9 54.9 25.9 37.5 59.6 45.31
OmegaNet(ours) 26.03 28.76 35.6 65.3 39.7 52.0 56.1 28.6 37.7 60.4 46.93
Atlas(ours) 25.88 28.54 36.1 64.9 40.1 52.7 56.4 28.8 38.1 61.2 47.28
```